[{"path":"/articles/discovery_replication.html","id":"addition-of-toy-replication-data-set","dir":"Articles","previous_headings":"","what":"Addition of toy replication data set","title":"Methods for use with discovery and replication GWASs","text":"","code":"set.seed(1998) n_snps <- 10^6 effect_snps <- 0.01*n_snps n_samples <- 30000 maf <- runif(n_snps,0.01,0.5) true_beta <- rnorm(effect_snps,0,1) h2 <- 0.7 # variance explained by effect SNPs var_y <- sum(2*maf[1:effect_snps]*(1-maf[1:effect_snps])*true_beta^2)/h2 true_beta <- true_beta/sqrt(var_y) # scaling to represent a phenotype with variance 1 true_beta <- c(true_beta, rep(0,n_snps-effect_snps)) se <- sqrt((1 - 2*maf*(1-maf)*true_beta^2)/(2*(n_samples-2)*maf*(1-maf))) stats_disc <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se),se=se)  n_samples_rep <- 300000 se_rep <- sqrt((1 - 2*maf*(1-maf)*true_beta^2)/(2*(n_samples_rep-2)*maf*(1-maf))) stats_rep <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se_rep),se=se_rep)  head(stats_disc) #>   rsid         beta          se #> 1    1 -0.014621777 0.024816486 #> 2    2  0.003693032 0.011860650 #> 3    3 -0.056895857 0.021248869 #> 4    4 -0.023764922 0.025252738 #> 5    5  0.028410276 0.008177783 #> 6    6 -0.001963061 0.009937206 head(stats_rep) #>   rsid         beta          se #> 1    1 -0.004533234 0.007847427 #> 2    2  0.002738190 0.003750554 #> 3    3 -0.019410836 0.006719281 #> 4    4 -0.019199787 0.007985377 #> 5    5  0.036047493 0.002585964 #> 6    6  0.002438338 0.003142326"},{"path":"/articles/discovery_replication.html","id":"method-1-conditional-likelihood","dir":"Articles","previous_headings":"","what":"Method 1: Conditional Likelihood","title":"Methods for use with discovery and replication GWASs","text":"function condlike_rep implements version conditional likelihood method obtaining bias-reduced estimates \\(\\beta\\) described Zhong Prentice (2008). function requires inputs two independent data sets, one representing discovery GWAS, summary_disc, replication study identical SNPs, summary_rep, well specification significance threshold, alpha, used. , data sets must form data frames columns rsid, beta se, columns data frames must contain numerical values row data frames must represent unique SNP, identified rsid. SNPs must exact order data sets, .e. identity summary_rep$rsid == summary_disc$rsid must evaluate TRUE. Furthermore, parameter conf_interval condlike_rep provides user option obtain confidence intervals adjusted association estimates. default setting conf_interval=FALSE. Inserting conf_interval=TRUE using function return three sets lower upper confidence interval boundaries SNP, set corresponding particular form adjusted estimate. final parameter conf_level takes numerical value 0 1 specifies confidence interval, default 0.95. similar manner functions included package, condlike_rep returns single data frame SNPs reordered based significance. However, contains SNPs deemed significant discovery data set. first 5 columns data frame details inputted information; rsid, beta_disc, se_disc, beta_rep, se_rep. Following , beta_com inverse variance weighted estimate formally defined :\\[\\hat\\beta_{\\text{com}} = \\frac{\\sigma_2^2 \\hat\\beta_1 + \\sigma_1^2 \\hat\\beta_2}{\\sigma_1^2 + \\sigma_2^2},\\] \\(\\hat\\beta_1\\) = beta_disc, \\(\\hat\\beta_2\\) = beta_rep, \\(\\sigma_1\\) = se_disc \\(\\sigma_2\\) = se_rep. method implemented uses just one selection cut-point first discovery stage opposed described Zhong Prentice (2008) two separate selection thresholds used. Thus, maximum likelihood adjusted estimator, beta_MLE defined maximize conditional likelihood observed \\(\\hat\\beta_{\\text{com}}\\):\\[\\hat\\beta_{\\text{MLE}} = \\arg \\max_{\\beta} \\log f(\\hat\\beta_{\\text{com}}; \\beta).\\] conditional sampling distribution, \\(f(x;\\beta)\\) approximated : \\[f(x;\\beta) = \\frac{\\frac{1}{\\sigma_{\\text{com}}} \\phi\\left(\\frac{x-\\beta}{\\sigma_{\\text{com}}}\\right) \\cdot \\left[\\Phi\\left(\\frac{x-c\\sigma_1}{\\frac{\\sigma_1}{\\sigma_2}\\sigma_{\\text{com}}}\\right) + \\Phi\\left(\\frac{-x-c\\sigma_1}{\\frac{\\sigma_1}{\\sigma_2}\\sigma_{\\text{com}}}\\right)\\right]}{\\Phi\\left(\\frac{\\beta}{\\sigma_1} - c\\right) + \\Phi\\left(- \\frac{\\beta}{\\sigma_1} - c\\right)}.\\] \\(c\\) selection cut-point, .e. SNPs \\(\\mid \\frac{\\hat\\beta_1}{\\sigma_1}\\mid \\ge c\\) deemed significant. value \\(c\\) easily obtained using chosen alpha. addition, \\[\\sigma^2_{\\text{com}} = \\frac{\\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_2^2}.\\] Note function, \\(f(x;\\beta)\\) slightly different given paper one selection cut-point imposed . Finally, Zhong Prectice (2008) noted simulation studies showed \\(\\hat\\beta_{\\text{com}}\\) tended upward bias \\(\\hat\\beta_{\\text{MLE}}\\) -corrected therefore, combination two following form proposed: \\[\\hat\\beta_{\\text{MSE}} = \\frac{\\hat\\sigma^2_{\\text{com}}\\cdot \\hat\\beta_{\\text{com}} + (\\hat\\beta_{\\text{com}} - \\hat\\beta_{\\text{MLE}})^2\\cdot\\hat\\beta_{\\text{MLE}}}{\\sigma^2_{\\text{com}}+(\\hat\\beta_{\\text{com}} - \\hat\\beta_{\\text{MLE}})^2}.\\] \\(\\hat\\beta_{\\text{MSE}}\\) holds final column outputted data frame default setting. use condlike_rep toy data sets conf_interval=FALSE demonstrated , significance threshold value 10e-6: \\(~\\) Confidence intervals: Firstly, \\((1-\\alpha)\\%\\) confidence interval \\(\\hat\\beta_{\\text{com}}\\) simply calculated : \\[\\hat\\beta_{\\text{com}} \\pm \\hat\\sigma_{\\text{com}}Z_{1-\\frac{\\alpha}{2}}.\\]\\(\\hat\\beta_{\\text{MLE}}\\), profile confidence limits intersection log-likelihood curve horizontal line \\(\\frac{\\chi^2_{1,1-\\alpha}}{2}\\) units maximum. MSE weighting method, described , can easily applied upper lower boundaries two confidence intervals obtain appropriate confidence interval \\(\\hat\\beta_{\\text{MSE}}\\). gives: \\[\\hat\\beta_{\\text{MSE};\\frac{\\alpha}{2}} = \\hat{K}_{\\frac{\\alpha}{2}} \\hat\\beta_{\\text{com};\\frac{\\alpha}{2}} + \\left(1-\\hat{K}_{\\frac{\\alpha}{2}}\\right) \\hat\\beta_{\\text{MLE};\\frac{\\alpha}{2}}\\] \\[\\hat\\beta_{\\text{MSE};1-\\frac{\\alpha}{2}} = \\hat{K}_{1-\\frac{\\alpha}{2}} \\hat\\beta_{\\text{com};1-\\frac{\\alpha}{2}} + \\left(1-\\hat{K}_{1-\\frac{\\alpha}{2}}\\right) \\hat\\beta_{\\text{MLE};1-\\frac{\\alpha}{2}}\\] \\(\\hat{K}_{\\frac{\\alpha}{2}} = \\frac{\\hat\\sigma^2_{\\text{com}}}{\\hat\\sigma^2_{\\text{com}} + \\left(\\hat\\beta_{\\text{com};\\frac{\\alpha}{2}} - \\hat\\beta_{\\text{MLE};\\frac{\\alpha}{2}}\\right)^2} \\;\\) \\(\\; \\hat{K}_{1-\\frac{\\alpha}{2}} = \\frac{\\hat\\sigma^2_{\\text{com}}}{\\hat\\sigma^2_{\\text{com}} + \\left(\\hat\\beta_{\\text{com};1-\\frac{\\alpha}{2}} - \\hat\\beta_{\\text{MLE};1-\\frac{\\alpha}{2}}\\right)^2}.\\) implement condlike_rep toy data sets conf_interval now set TRUE show form output now takes. similar data frame returned 95% confidence intervals also included adjusted association estimate SNP. Note: current computation confidence intervals may work every situation thus, ongoing work done resolve .","code":"out1 <- condlike_rep(summary_disc=stats_disc, summary_rep=stats_rep, alpha=10e-6) head(out1) #>   rsid   beta_disc     se_disc    beta_rep      se_rep    beta_com    beta_MLE #> 1 3965  0.06064760 0.008191123  0.05034712 0.002590183  0.05128348  0.05121019 #> 2 7815  0.06273204 0.008477889  0.04711212 0.002680864  0.04853203  0.04836559 #> 3 4998 -0.05956742 0.008336576 -0.04797101 0.002636178 -0.04902517 -0.04890038 #> 4 7261  0.05510736 0.008186396  0.05423749 0.002588688  0.05431656  0.05425300 #> 5 6510  0.05623389 0.008389116  0.05314707 0.002652792  0.05342768  0.05335028 #> 6 9917  0.05466029 0.008176299  0.04281451 0.002585495  0.04389134  0.04365907 #>      beta_MSE #> 1  0.05128341 #> 2  0.04853133 #> 3 -0.04902487 #> 4  0.05431652 #> 5  0.05342761 #> 6  0.04388930 out1_conf <- condlike_rep(summary_disc=stats_disc, summary_rep=stats_rep, alpha=10e-6, conf_interval=TRUE, conf_level=0.95) head(out1_conf) #>   rsid   beta_disc     se_disc    beta_rep      se_rep    beta_com #> 1 3965  0.06064760 0.008191123  0.05034712 0.002590183  0.05128348 #> 2 7815  0.06273204 0.008477889  0.04711212 0.002680864  0.04853203 #> 3 4998 -0.05956742 0.008336576 -0.04797101 0.002636178 -0.04902517 #> 4 7261  0.05510736 0.008186396  0.05423749 0.002588688  0.05431656 #> 5 6510  0.05623389 0.008389116  0.05314707 0.002652792  0.05342768 #> 6 9917  0.05466029 0.008176299  0.04281451 0.002585495  0.04389134 #>   beta_com_lower beta_com_upper    beta_MLE beta_MLE_lower beta_MLE_upper #> 1     0.04644305     0.05612390  0.05121019     0.04634750     0.05608599 #> 2     0.04352215     0.05354192  0.04836559     0.04329691     0.05344293 #> 3    -0.05395155    -0.04409880 -0.04890038    -0.05387616    -0.04392062 #> 4     0.04947893     0.05915419  0.05425300     0.04943259     0.05913523 #> 5     0.04847025     0.05838510  0.05335028     0.04838881     0.05835422 #> 6     0.03905968     0.04872300  0.04365907     0.03872583     0.04855722 #>      beta_MSE beta_MSE_lower beta_MSE_upper #> 1  0.05128341     0.04644291     0.05612389 #> 2  0.04853133     0.04352041     0.05354177 #> 3 -0.04902487    -0.05395148    -0.04409791 #> 4  0.05431652     0.04947892     0.05915419 #> 5  0.05342761     0.04847017     0.05838510 #> 6  0.04388930     0.03905366     0.04872226"},{"path":"/articles/discovery_replication.html","id":"method-2-umvcue","dir":"Articles","previous_headings":"","what":"Method 2: UMVCUE","title":"Methods for use with discovery and replication GWASs","text":"implementation UMVCUE similar function described sense UMVCUE requires inputs; discovery replication data sets form three-columned data frames together threshold value, alpha. Furthermore, outputted data frame form just one extra column providing adjusted estimate, beta_UMVCUE. Selection also occurs just one stage - SNPs deemed significant \\(p\\)-values corresponding \\(\\mid \\frac{\\hat\\beta_1}{\\sigma_1}\\mid\\) smaller given threshold. function UMVCUE executes method detailed Bowden Dudbridge (2009). adaptations made method described. worth noting , conditional likelihood methods, method used condlike_rep makes adjustments SNP one time information relating SNPs required adjustment. However, ordering SNPs based significance, single SNP, UMVCUE also uses data SNPs either side assist adjustment. UMVCUE can applied toy data sets followed, alpha specified 10e-6:","code":"out2 <- UMVCUE(summary_disc = stats_disc, summary_rep = stats_rep, alpha = 10e-6) head(out2) #>   rsid   beta_disc     se_disc    beta_rep      se_rep beta_UMVCUE #> 1 3965  0.06064760 0.008191123  0.05034712 0.002590183  0.04996931 #> 2 7815  0.06273204 0.008477889  0.04711212 0.002680864  0.04722594 #> 3 4998 -0.05956742 0.008336576 -0.04797101 0.002636178 -0.04807677 #> 4 7261  0.05510736 0.008186396  0.05423749 0.002588688  0.05408398 #> 5 6510  0.05623389 0.008389116  0.05314707 0.002652792  0.05314277 #> 6 9917  0.05466029 0.008176299  0.04281451 0.002585495  0.04282613"},{"path":"/articles/discovery_replication.html","id":"method-3-mse-minimization","dir":"Articles","previous_headings":"","what":"Method 3: MSE minimization","title":"Methods for use with discovery and replication GWASs","text":"function MSE_minimizer implements combination method closely follows described Ferguson et al. (2017). function parameters used ; summary_disc, summary_rep alpha, precisely form previously detailed vignette. addition, MSE_minimizer logical parameter, namely spline defaults spline=TRUE. smoothing spline used execution function, data corresponding least 5 SNPs required. adjusted estimate computed SNP classified significant discovery data set, based given threshold. Thus, similar method, MSE_minimizer returns data frame containing significant SNPs 6 columns final column contains new estimate, beta_joint. Following approach detailed Ferguson et al. (2017), define adjusted linear combination estimator : \\[\\hat\\beta_{\\text{joint}} = \\omega(\\hat{B}) \\cdot \\hat\\beta_{\\text{rep}} + (1-\\omega(\\hat{B}))\\cdot \\hat\\beta_{\\text{disc}}\\] \\[ \\omega(\\hat{B}) = \\frac{\\frac{1}{\\sigma^2_{\\text{rep}}}}{\\frac{1}{\\sigma^2_{\\text{rep}}}+\\frac{1}{\\sigma^2_{\\text{disc}}+ \\hat{B}^2}}.\\] spline=FALSE used, simply let \\(\\hat{B} = \\hat\\beta_{\\text{disc}} - \\hat\\beta_{\\text{rep}}\\). make assumptions \\(\\beta_{\\text{rep}}\\) unbiased \\(\\beta\\), \\(\\beta_{\\text{disc}}\\) quite likely biased \\(\\beta_{\\text{rep}}\\) \\(\\beta_{\\text{disc}}\\) independent. default setting spline=TRUE, cubic smoothing spline applied values \\(z_{\\text{disc}} = \\frac{\\hat\\beta_{\\text{disc}}}{\\sigma_{\\text{disc}}}\\) considered inputs \\(\\hat{B} = \\hat\\beta_{\\text{disc}} - \\hat\\beta_{\\text{rep}}\\), corresponding outputs. predicted values \\(\\hat{B}\\) process, \\(\\hat{B}^*\\) say, used instead \\(\\hat{B}\\) computing \\(\\hat\\beta_{\\text{joint}}\\) SNP. apply MSE_minimizer toy data sets, default setting spline spline=FALSE. convenient demonstration purposes, specify significance threshold 10e-6.","code":"out3 <- MSE_minimizer(summary_disc = stats_disc, summary_rep = stats_rep, alpha=10e-6, spline=FALSE) out4 <- MSE_minimizer(summary_disc = stats_disc, summary_rep = stats_rep, alpha=10e-6)  head(out3) #>   rsid   beta_disc     se_disc    beta_rep      se_rep  beta_joint #> 1 3965  0.06064760 0.008191123  0.05034712 0.002590183  0.05073125 #> 2 7815  0.06273204 0.008477889  0.04711212 0.002680864  0.04745963 #> 3 4998 -0.05956742 0.008336576 -0.04797101 0.002636178 -0.04835308 #> 4 7261  0.05510736 0.008186396  0.05423749 0.002588688  0.05431576 #> 5 6510  0.05623389 0.008389116  0.05314707 0.002652792  0.05339693 #> 6 9917  0.05466029 0.008176299  0.04281451 0.002585495  0.04318478 head(out4) #>   rsid   beta_disc     se_disc    beta_rep      se_rep  beta_joint #> 1 3965  0.06064760 0.008191123  0.05034712 0.002590183  0.05053320 #> 2 7815  0.06273204 0.008477889  0.04711212 0.002680864  0.04741219 #> 3 4998 -0.05956742 0.008336576 -0.04797101 0.002636178 -0.04806024 #> 4 7261  0.05510736 0.008186396  0.05423749 0.002588688  0.05426883 #> 5 6510  0.05623389 0.008389116  0.05314707 0.002652792  0.05326308 #> 6 9917  0.05466029 0.008176299  0.04281451 0.002585495  0.04324965"},{"path":"/articles/discovery_replication.html","id":"visualisation","dir":"Articles","previous_headings":"","what":"Visualisation","title":"Methods for use with discovery and replication GWASs","text":"can illustrate performance three functions; condlike_rep, UMVCUE MSE_minimizer, follows. clear methods show improvement estimates obtained discovery data set. However, plots show investigation required order evaluate adjusted estimates less biased mere use replication estimates. second graph, can seen must little difference replication estimates obtained using UMVCUE density curves beta_rep beta_UMVCUE nearly overlap completely.   addition, data sets simulated, obtain measures sum squared differences mean absolute differences various estimates follows:","code":"library(\"RColorBrewer\") col <- brewer.pal(8,\"Dark2\")  plot(density(abs(out1$beta_disc)),ylim=c(0,68),xlim=c(-0.01,0.08),main=\"Conditional Likelihood Winner's Curse Adjustment\",col=col[1],lwd=2) lines(density(abs(true_beta[out1$rsid])),col=col[2],lwd=2) lines(density(abs(out1$beta_rep)),col=col[3],lwd=2) lines(density(abs(out1$beta_com)),col=col[4],lwd=2) lines(density(abs(out1$beta_MLE)),col=col[5],lwd=2) lines(density(abs(out1$beta_MSE)),col=col[8],lwd=2) legend(0.05, 67.5, legend=c(\"beta_disc\", \"true_beta\",\"beta_rep\",\"beta_com\", \"beta_MLE\", \"beta_MSE\"),col=c(col[1:5],col[8]),lty=1,lwd=2) plot(density(abs(out2$beta_disc)),ylim=c(0,72),xlim=c(-0.01,0.08),main=\"UMVCUE and MSE-minimizer Winner's Curse Adjustment\",col=col[1],lwd=2) lines(density(abs(true_beta[out2$rsid])),col=col[2],lwd=2) lines(density(abs(out2$beta_rep)),col=col[3],lwd=2) lines(density(abs(out2$beta_UMVCUE)),col=col[4],lwd=2) lines(density(abs(out3$beta_joint)),col=col[5],lwd=2) lines(density(abs(out4$beta_joint)),col=col[6],lwd=2) legend(0.05, 70.5, legend=c(\"beta_disc\", \"true_beta\",\"beta_rep\",\"beta_UMVCUE\", \"beta_joint\", \"beta_joint_sp\"),        col=col,lty=1,lwd=2) sq_diff <- data.frame(disc_naive = sum((true_beta[out2$rsid] - out2$beta_disc)^2), rep_naive = sum((true_beta[out2$rsid] - out2$beta_rep)^2),  beta_UMVCUE = sum((true_beta[out2$rsid] - out2$beta_UMVCUE)^2), beta_com = sum((true_beta[out1$rsid] - out1$beta_com)^2), beta_MLE = sum((true_beta[out1$rsid] - out1$beta_MLE)^2), beta_MSE = sum((true_beta[out1$rsid] - out1$beta_MSE)^2), beta_joint = sum((true_beta[out3$rsid] - out3$beta_joint)^2), beta_joint_sp = sum((true_beta[out4$rsid] - out4$beta_joint)^2)) sq_diff #>   disc_naive   rep_naive beta_UMVCUE    beta_com   beta_MLE    beta_MSE #> 1 0.06867092 0.001304645 0.001276025 0.001708696 0.00131269 0.001470638 #>    beta_joint beta_joint_sp #> 1 0.001302419   0.001357969  mean_abs_diff <- data.frame(disc_naive = mean(abs(true_beta[out2$rsid] - out2$beta_disc)), rep_naive = mean(abs(true_beta[out2$rsid] - out2$beta_rep)),  beta_UMVCUE = mean(abs(true_beta[out2$rsid] - out2$beta_UMVCUE)), beta_com = mean(abs(true_beta[out1$rsid] - out1$beta_com)),beta_MLE = mean(abs(true_beta[out1$rsid] - out1$beta_MLE)), beta_MSE = mean(abs(true_beta[out1$rsid] - out1$beta_MSE)), beta_joint = mean(abs(true_beta[out3$rsid] - out3$beta_joint)), beta_joint_sp = mean(abs(true_beta[out4$rsid] - out4$beta_joint))) mean_abs_diff #>   disc_naive   rep_naive beta_UMVCUE   beta_com  beta_MLE    beta_MSE #> 1 0.01593473 0.002368839 0.002328191 0.00269438 0.0023679 0.002559311 #>    beta_joint beta_joint_sp #> 1 0.002373599   0.002444096  frac_less_bias <- data.frame(beta_UMVCUE = (sum(abs(true_beta[out2$rsid] - out2$beta_rep) > abs(true_beta[out2$rsid] - out2$beta_UMVCUE)))/nrow(out2), beta_com = (sum(abs(true_beta[out1$rsid] - out1$beta_rep) > abs(true_beta[out1$rsid] - out1$beta_com)))/nrow(out1), beta_MLE = (sum(abs(true_beta[out1$rsid] - out1$beta_rep) > abs(true_beta[out1$rsid] - out1$beta_MLE)))/nrow(out1), beta_MSE = (sum(abs(true_beta[out1$rsid] - out1$beta_rep) > abs(true_beta[out1$rsid] - out1$beta_MSE)))/nrow(out1), beta_joint = (sum(abs(true_beta[out3$rsid] - out3$beta_rep) > abs(true_beta[out3$rsid] - out3$beta_joint)))/nrow(out3), beta_joint_sp = (sum(abs(true_beta[out4$rsid] - out4$beta_rep) > abs(true_beta[out4$rsid] - out4$beta_joint)))/nrow(out4)) frac_less_bias  # comparison with naive replication estimate #>   beta_UMVCUE  beta_com  beta_MLE  beta_MSE beta_joint beta_joint_sp #> 1   0.4802632 0.4276316 0.5131579 0.4605263        0.5     0.4934211"},{"path":"/articles/discovery_replication.html","id":"using-empirical-bayes-method-when-a-replication-data-set-is-available","dir":"Articles","previous_headings":"","what":"Using Empirical Bayes method when a replication data set is available","title":"Methods for use with discovery and replication GWASs","text":"replication data set also available, described throughout document, also option employ empirical Bayes method. seen work considerably well simulations thus advise exploration implementation setting. data form described similar stats_disc stats_rep, implementation empirical Bayes method take place combined estimator shown : compute evaluation metrics considered well create plot depicting results.","code":"com_stats <- stats_disc   com_stats$beta <- ((((stats_rep$se)^2)*(stats_disc$beta))+(((stats_disc$se)^2)*(stats_rep$beta)))/(((stats_disc$se)^2) + ((stats_rep$se)^2))   com_stats$se <- sqrt((((stats_disc$se)^2)*((stats_rep$se)^2))/(((stats_disc$se)^2) + ((stats_rep$se)^2)))   out_EB_com <- empirical_bayes(com_stats)   out_EB_com <- dplyr::arrange(out_EB_com,out_EB_com$rsid)   out_EB_com <- data.frame(rsid = stats_disc$rsid, beta_disc = stats_disc$beta, se_disc  = stats_disc$se, beta_rep = stats_rep$beta, se_rep = stats_rep$se, beta_EB=out_EB_com$beta_EB)    ## reorder based on significance in first set up!   out_EB_com <- dplyr::arrange(out_EB_com, dplyr::desc(abs(out_EB_com$beta_disc/out_EB_com$se_disc)))   out_EB_com <- out_EB_com[abs(out_EB_com$beta_disc/out_EB_com$se_disc) > stats::qnorm(1-(10e-6)/2),]   head(out_EB_com) #>   rsid   beta_disc     se_disc    beta_rep      se_rep     beta_EB #> 1 3965  0.06064760 0.008191123  0.05034712 0.002590183  0.05099432 #> 2 7815  0.06273204 0.008477889  0.04711212 0.002680864  0.04823275 #> 3 4998 -0.05956742 0.008336576 -0.04797101 0.002636178 -0.04800400 #> 4 7261  0.05510736 0.008186396  0.05423749 0.002588688  0.05402757 #> 5 6510  0.05623389 0.008389116  0.05314707 0.002652792  0.05313153 #> 6 9917  0.05466029 0.008176299  0.04281451 0.002585495  0.04360270 sq_diff <- data.frame(disc_naive = sum((true_beta[out2$rsid] - out2$beta_disc)^2), rep_naive = sum((true_beta[out2$rsid] - out2$beta_rep)^2),  beta_EB_joint = sum((true_beta[out_EB_com$rsid] - out_EB_com$beta_EB)^2)) sq_diff #>   disc_naive   rep_naive beta_EB_joint #> 1 0.06867092 0.001304645   0.001203244  mean_abs_diff <- data.frame(disc_naive = mean(abs(true_beta[out2$rsid] - out2$beta_disc)), rep_naive = mean(abs(true_beta[out2$rsid] - out2$beta_rep)),  beta_EB_joint = mean(abs(true_beta[out_EB_com$rsid] - out_EB_com$beta_EB))) mean_abs_diff #>   disc_naive   rep_naive beta_EB_joint #> 1 0.01593473 0.002368839   0.002205367  frac_less_bias <- data.frame(beta_EB_joint = (sum(abs(true_beta[out_EB_com$rsid] - out_EB_com$beta_rep) > abs(true_beta[out_EB_com$rsid] - out_EB_com$beta_EB)))/nrow(out_EB_com)) frac_less_bias  # comparison with naive replication estimate #>   beta_EB_joint #> 1     0.5723684 plot(density(abs(out_EB_com$beta_disc)),ylim=c(0,72),xlim=c(-0.01,0.08),main=\"Empirical Bayes Winner's Curse Adjustment\",col=col[1],lwd=2) lines(density(abs(true_beta[out_EB_com$rsid])),col=col[2],lwd=2) lines(density(abs(out_EB_com$beta_rep)),col=col[3],lwd=2) lines(density(abs(out_EB_com$beta_EB)),col=col[4],lwd=2) legend(0.05, 70.5, legend=c(\"beta_disc\", \"true_beta\",\"beta_rep\",\"beta_EB_joint\"),        col=col,lty=1,lwd=2)"},{"path":"/articles/standard_errors_confidence_intervals.html","id":"obtaining-standard-errors-of-adjusted-estimates","dir":"Articles","previous_headings":"","what":"Obtaining standard errors of adjusted estimates","title":"Standard errors and confidence intervals of adjusted estimates","text":"function se_adjust three parameters: summary_data: data frame form described previously, columns rsid, beta se method: user required specify method wish implement order obtain standard errors; \"empirical_bayes\", \"BR_ss\", \"FDR_IQT\" n_boot: numerical value defines number bootstraps used, default option 100 bootstraps. function requires number greater 1. se_adjust outputs data frame similar format functions included package. includes beta estimates obtained adjustment using specified method well additional column adj_se. column contains value approximates standard error adjusted estimate \\(\\beta\\). se_adjust uses parametric bootstrap approach. Firstly, using inputted data set, n_boot individual data sets created follows. bootstrap \\(\\text{b} = 1, \\dots\\)n_boot, value \\(\\hat\\beta^{(\\text{b})}\\) generated SNP normal distribution centred naive \\(\\hat\\beta\\) standard error, \\(\\text{se}(\\hat\\beta)\\): \\[\\hat\\beta^{(\\text{b})} \\sim N(\\hat\\beta, \\text{se}(\\hat\\beta)).\\] specified method implemented n_boot data sets SNP, now n_boot adjusted ‘bootstrapped’ \\(\\hat\\beta\\) estimates. standard error easily obtained applying sd set adjusted estimates SNP. important note use bootstrapping large data sets can quite computationally intensive. personal computer, executing 100 bootstraps can result se_adjust taking 1 3 minutes provide output data set one described . expected, BR_ss tends take longest three methods. example implementation function three methods given . ease demonstration, mere 10 bootstraps used occasions. However, value n_boot small advised practice. Due nature conditional likelihood methods, obtain standard errors adjusted beta estimates manner.","code":"out_EB <- se_adjust(summary_data = summary_stats, method = \"empirical_bayes\", n_boot=10) head(out_EB) #>   rsid        beta          se     beta_EB      adj_se #> 1 3965  0.06064760 0.008191123  0.05177699 0.012056841 #> 2 7815  0.06273204 0.008477889  0.05355088 0.012128193 #> 3 4998 -0.05956742 0.008336576 -0.05107993 0.013921651 #> 4 7261  0.05510736 0.008186396  0.04624187 0.012507416 #> 5 6510  0.05623389 0.008389116  0.04714887 0.013614514 #> 6 9917  0.05466029 0.008176299  0.04580573 0.008606223  out_BR_ss <- se_adjust(summary_data = summary_stats, method = \"BR_ss\", n_boot=10) head(out_BR_ss) #>   rsid        beta          se  beta_BR_ss      adj_se #> 1 3965  0.06064760 0.008191123  0.04608950 0.009319973 #> 2 7815  0.06273204 0.008477889  0.04765488 0.009184917 #> 3 4998 -0.05956742 0.008336576 -0.04363663 0.004012381 #> 4 7261  0.05510736 0.008186396  0.03911355 0.005791631 #> 5 6510  0.05623389 0.008389116  0.03977873 0.006310246 #> 6 9917  0.05466029 0.008176299  0.03858223 0.005714822  out_FIQT <- se_adjust(summary_data = summary_stats, method = \"FDR_IQT\", n_boot=10) head(out_FIQT) #>   rsid        beta          se   beta_FIQT      adj_se #> 1 3965  0.06064760 0.008191123  0.04419398 0.015086292 #> 2 7815  0.06273204 0.008477889  0.04574119 0.017748660 #> 3 4998 -0.05956742 0.008336576 -0.04271559 0.019351045 #> 4 7261  0.05510736 0.008186396  0.03781859 0.008410062 #> 5 6510  0.05623389 0.008389116  0.03875509 0.006059352 #> 6 9917  0.05466029 0.008176299  0.03777195 0.013878633"},{"path":"/articles/standard_errors_confidence_intervals.html","id":"confidence-intervals-for-estimates-adjusted-by-conditional-likelihood-methods","dir":"Articles","previous_headings":"","what":"Confidence intervals for estimates adjusted by conditional likelihood methods","title":"Standard errors and confidence intervals of adjusted estimates","text":"function cl_interval implements conditional likelihood methods described Ghosh et al. (2008) significant SNP together three adjusted estimates, provides single confidence interval SNP. well data set form data frame columns rsid, beta se, cl_interval requires alpha, value specifies significance threshold conf, value 0 1 determines confidence interval. default setting conf 0.95. Note: order function output appropriate confidence interval significant SNP, absolute value largest \\(z\\)-statistic data set must less 150. confidence interval adjusted estimate specified Ghosh et al. (2008). \\(\\mu = \\frac{\\beta}{\\text{se}(\\beta)}\\) \\(z = \\frac{\\hat\\beta}{\\hat{\\text{se}(\\hat\\beta)}}\\), consider acceptance region \\((\\mu,1-\\eta)\\), defined interval \\(\\eta/2\\) \\(1-\\eta/2\\) quantiles conditional density, \\(p_{\\mu}(z | \\mid Z \\mid > c )\\). obtained lower upper boundaries acceptance region, desired confidence interval given : \\[(\\mu_{\\text{lower}}\\hat{\\text{se}(\\hat\\beta)}, \\mu_{\\text{upper}}\\hat{\\text{se}(\\hat\\beta)}).\\] output cl_interval, lower column holds values lower confidence limit SNP upper column contains upper confidence limit. cl_interval can used conjunction toy data set shown , alpha specified 5e-8 95% confidence interval desired: \\(~\\) \\(~\\) : Creating confidence intervals three methods noted adjusted estimates BR_ss normally distributed - one option obtain quantiles parametric bootstrap, similar implementation se_adjust computationally intensive?","code":"out <- cl_interval(summary_data=summary_stats, alpha = 5e-8, conf_level=0.95)"},{"path":"/articles/winners_curse_methods.html","id":"creating-a-toy-data-set","dir":"Articles","previous_headings":"","what":"Creating a toy data set","title":"Methods for use with discovery GWAS","text":"wish generate summary statistics 1 million SNPs, exists polygenic background 10,000 SNPs others effects. specify: total number SNPs number SNPs truly associated trait number samples minor allele frequency SNP variance explained trait SNPs, \\(h^2\\) specifications, can obtain quite realistic summary statistics shown . summary statistics arranged suitable way method - form data frame columns rsid, beta se.","code":"set.seed(1998)  n_snps <- 10^6  n_samples <- 30000 effect_snps <- 0.01*n_snps  maf <- runif(n_snps,0.01,0.5)  true_beta <- rnorm(effect_snps,0,1) h2 <- 0.7 # variance explained by effect SNPs var_y <- sum(2*maf[1:effect_snps]*(1-maf[1:effect_snps])*true_beta^2)/h2  true_beta <- true_beta/sqrt(var_y) # scaling to represent a phenotype with variance 1 true_beta <- c(true_beta, rep(0,n_snps-effect_snps)) se <- sqrt((1 - 2*maf*(1-maf)*true_beta^2)/(2*(n_samples-2)*maf*(1-maf)))    summary_stats <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se),se=se)     head(summary_stats) #>   rsid         beta          se #> 1    1 -0.014621777 0.024816486 #> 2    2  0.003693032 0.011860650 #> 3    3 -0.056895857 0.021248869 #> 4    4 -0.023764922 0.025252738 #> 5    5  0.028410276 0.008177783 #> 6    6 -0.001963061 0.009937206"},{"path":"/articles/winners_curse_methods.html","id":"method-1-conditional-likelihood","dir":"Articles","previous_headings":"","what":"Method 1: Conditional Likelihood","title":"Methods for use with discovery GWAS","text":"function conditional_likelihood requires data frame, form described , value alpha, significance threshold used GWAS. Note columns data frame must contain numerical values row must represent unique SNP, identified rsid. conditional likelihood methods designed used SNPs deemed significant, data frame returned contains SNPs \\(p\\)-values inputted genome-wide significance threshold value, alpha. Note: order function output appropriate value second form adjusted estimate, namely beta.cl2, significant SNP, absolute value largest \\(z\\)-statistic data set must less 150. SNPs detected significant, function merely returns inputted data frame. returned data frame SNPs ordered based \\(p\\)-values smallest largest, equivalently, descending order \\(\\mid z \\mid\\) \\(z\\) estimated value \\(\\beta\\) divided standard error. implement function toy data set follows, choosing significance threshold 5e-8: Three columns added right inputted data frame: represents correction method suggested Ghosh et al. (2008). first, beta.cl1, referred conditional MLE second, beta.cl2, mean normalized conditional likelihood. third, beta.cl3 merely average first two, known compromise estimator. Ghosh et al. (2008) noted instances true \\(\\beta\\) value close zero, beta.cl2 often greater mean squared error beta.cl1 true \\(\\beta\\) values zero, beta.cl2 performs better. Thus, suggested beta.cl3 used combine strengths two estimators. However, function, conditional_likelihood, outputs values three estimators order allow user make decision believe appropriate.","code":"out_CL <- conditional_likelihood(summary_data = summary_stats, alpha = 5e-8)  head(out_CL) #>   rsid        beta          se    beta.cl1    beta.cl2    beta.cl3 #> 1 3965  0.06064760 0.008191123  0.06007537  0.05868216  0.05937877 #> 2 7815  0.06273204 0.008477889  0.06213363  0.06068292  0.06140827 #> 3 4998 -0.05956742 0.008336576 -0.05853885 -0.05655174 -0.05754529 #> 4 7261  0.05510736 0.008186396  0.05271740  0.04940315  0.05106028 #> 5 6510  0.05623389 0.008389116  0.05363848  0.05011923  0.05187885 #> 6 9917  0.05466029 0.008176299  0.05203604  0.04852832  0.05028218"},{"path":"/articles/winners_curse_methods.html","id":"method-2-empirical-bayes","dir":"Articles","previous_headings":"","what":"Method 2: Empirical Bayes","title":"Methods for use with discovery GWAS","text":"function empirical_bayes implements Empirical Bayes method correcting Winner’s Curse, detailed Ferguson et al. (2013), slight modification. function one argument, summary_data, data frame containing rsid, beta se columns. , columns data frame must contain numerical values row must represent unique SNP, identified rsid. addition, function requires data frame must contain least 50 rows corresponding 50 unique SNPs. requirement included Empirical Bayes method performs poorly data small number SNPs available. returns described data frame fourth column, beta_EB adjusted estimates procedure added. Empirical Bayes method makes adjustments SNPs, considered significant, data frame contains SNPs. Note: conditional likelihood methods adjust statistic one time, summary statistics one individual SNP can easily inputted conditional_likelihood function. However, Empirical Bayes method requires many SNPs information given function, accurate modelling distribution thus, making adjustments. Thus, mentioned , summary_data must contain information related 50 SNPs. demonstration using empirical_bayes toy data set: Unfortunately, Empirical Bayes estimator described Ferguson et al. (2013) known perform poorly extreme tails distribution. Therefore, quite possible lack adjustment \\(\\beta\\) values SNPs extreme \\(z\\)-statistics witnessed method. somewhat ad hoc approach overcome issue combining Empirical Bayes conditional likelihood methods suggested Ferguson et al. (2013). order ensure shrinkage occur \\(\\beta\\) values extreme SNPs, decided following modification added empirical_bayes function detailed . basis function natural cubic spline altered boundary knots longer extreme \\(z\\)-values. Instead, lower boundary knot defined \\(10^{\\text{th}}\\) \\(z\\)-statistic \\(z\\)-statistics lie increasing order upper boundary knot \\(10^{\\text{th}}\\) \\(z\\)-statistic \\(z\\)-statistics arranged decreasing order. natural cubic spline merely linear beyond boundary knots. assists fixing ‘tails’ issue discussed . see reduction estimated \\(\\beta\\) values top 6 significant SNPs occurred.","code":"out_EB <- empirical_bayes(summary_data = summary_stats) head(out_EB) #>   rsid        beta          se     beta_EB #> 1 3965  0.06064760 0.008191123  0.05177699 #> 2 7815  0.06273204 0.008477889  0.05355088 #> 3 4998 -0.05956742 0.008336576 -0.05107993 #> 4 7261  0.05510736 0.008186396  0.04624187 #> 5 6510  0.05623389 0.008389116  0.04714887 #> 6 9917  0.05466029 0.008176299  0.04580573"},{"path":"/articles/winners_curse_methods.html","id":"method-3-fdr-inverse-quantile-transformation","dir":"Articles","previous_headings":"","what":"Method 3: FDR Inverse Quantile Transformation","title":"Methods for use with discovery GWAS","text":"function FDR_IQT implements winner’s curse adjustment method detailed Bigdeli et al. (2016). Similar approaches , function requires data frame three columns rsid, beta se, columns data frame must contain numerical values row must represent unique SNP, identified rsid. also argument min_pval, default setting 1e-15. included order avoid zero \\(p\\)-values can prove problematic function evaluating qnorm(). Furthermore, due nature winner’s curse, fact rare observations \\(\\mid z \\mid > 8\\) biased. function outputs data frame similar inputted additional column containing adjusted estimate, beta_FIQT, , SNPs reordered according individual \\(z\\)-statistics.","code":"out_FIQT <- FDR_IQT(summary_data = summary_stats) head(out_FIQT) #>   rsid        beta          se   beta_FIQT #> 1 3965  0.06064760 0.008191123  0.04419398 #> 2 7815  0.06273204 0.008477889  0.04574119 #> 3 4998 -0.05956742 0.008336576 -0.04271559 #> 4 7261  0.05510736 0.008186396  0.03781859 #> 5 6510  0.05623389 0.008389116  0.03875509 #> 6 9917  0.05466029 0.008176299  0.03777195"},{"path":"/articles/winners_curse_methods.html","id":"method-4-bootstrap","dir":"Articles","previous_headings":"","what":"Method 4: Bootstrap","title":"Methods for use with discovery GWAS","text":"Inspired winner’s curse adjustment method detailed Faye et al. (2011), function BR_ss implements computationally efficient bootstrap method suitable use summary statistics. BR_ss takes data frame summary_data three columns: rsid, beta, se, returns inputted data frame along adjusted estimate \\(\\beta\\), beta_BR_ss. Note columns data frame, summary data must contain numerical values, row must represent unique SNP, identified rsid function requires must least 5 unique SNPs smoothing spline can implemented successfully. function also logical parameter seed_opt can set TRUE user permit reproducibility. default setting seed_opt=FALSE. method involves bootstrap implementation, use seed can allow user obtain exact results beta_BR_ss running function many times dataset. parameter seed provides user option set desired value seed using seed_opt=TRUE. default seed arbitrary value 1998. \\(~\\) Method description: First, \\(N\\) SNPs arranged according naive \\(z\\)-statistics, \\(\\frac{\\hat\\beta}{\\text{se}(\\hat\\beta)}\\), descending order given rank \\(k\\). Thus, SNP largest positive original \\(z\\)-statistic given rank \\(k=1\\) SNP largest negative original \\(z\\)-statistic receives rank \\(k=N\\). Following , parametric bootstrap performed value \\(\\hat\\beta^{\\text{b}}_{(k)}\\) simulated SNP \\(k\\) distribution: \\[\\hat\\beta^{\\text{b}}_{(k)} \\sim N(\\hat\\beta_{(k)}, \\text{se}(\\hat\\beta_{(k)})).\\] Upon obtaining \\(\\hat\\beta^{\\text{b}}\\), SNPs undergo second ordering arranged based \\(z^{\\text{b}}\\)-statistics receive rank \\((k)\\) accordingly. \\(z^{\\text{b}}_{(k)}\\)-statistic SNP \\(k\\) equal \\(\\frac{\\hat\\beta^{\\text{b}}_{(k)}}{\\text{se}(\\hat\\beta_{(k)})}\\). estimate bias correction value SNP \\(k\\) given : \\[\\text{bias}_{(k)} = \\frac{\\hat\\beta^{\\text{b}}_{(k)} - \\hat\\beta^{\\text{oob}}_{(k)}}{\\text{se}(\\hat\\beta_{(k)})} = \\frac{\\hat\\beta^{\\text{b}}_{(k)} - \\hat\\beta_{(k)}}{\\text{se}(\\hat\\beta_{(k)})}, \\]\\(\\hat\\beta^{\\text{b}}_{(k)}\\) bootstrap value SNP ranked position \\(k\\) second ordering, \\(\\hat\\beta^{\\text{oob}}_{(k)} = \\hat\\beta_{(k)}\\) SNP’s naive \\(\\beta\\) estimate \\(\\text{se}(\\hat\\beta_{(k)})\\) ’s standard error. Next, cubic smoothing spline fitted data using stats::smooth.spline \\(z_{(k)}\\)-statistics considered inputs \\(\\text{bias}_{(k)}\\), corresponding outputs. predicted values process provides \\(\\text{bias}^{*}_{(k)}\\) SNP. Using \\(\\text{bias}^{*}_{(k)}\\) instead \\(\\text{bias}_{(k)}\\) results faster approach increased accuracy one bootstrap iteration SNP sufficient competitive performance. Finally, new estimate \\(\\beta\\) SNP \\(k\\) defined follows: \\[\\hat\\beta_{(k)}^{*} = \\hat\\beta_{(k)} - \\text{se}(\\hat\\beta_{(k)}) \\cdot  \\text{bias}^{*}_{(k)}.\\]","code":"out_BR_ss <- BR_ss(summary_data = summary_stats, seed=2020) head(out_BR_ss) #>   rsid        beta          se  beta_BR_ss #> 1 3965  0.06064760 0.008191123  0.04959912 #> 2 7815  0.06273204 0.008477889  0.05128017 #> 3 4998 -0.05956742 0.008336576 -0.04600100 #> 4 7261  0.05510736 0.008186396  0.04168222 #> 5 6510  0.05623389 0.008389116  0.04237417 #> 6 9917  0.05466029 0.008176299  0.04108921"},{"path":"/articles/winners_curse_methods.html","id":"comparing-results","dir":"Articles","previous_headings":"","what":"Comparing Results","title":"Methods for use with discovery GWAS","text":"simulated data set, ability quantify amount bias present original naive \\(\\beta\\) estimates also, adjusted estimates method. can use measures sum squared differences mean absolute differences assess bias. Furthermore, can also simply obtain fraction SNPs now less biased adjustment. First, take look conditional likelihood method follows, SNPs originally deemed significant included: first two quantities, values three conditional likelihood methods less naive estimate find least 62% significant SNPs now less biased. Next, investigate three methods described take account SNPs, just \\(p\\)-values less specified threshold. , see top two cases, value associated naive estimate greatest. assures us new adjusted estimates provided method indeed less biased original \\(\\beta\\) estimate. fact, method, least 99% association estimates SNPs now less biased. Finally, can compare performance methods SNPs raw \\(p\\)-values less 5e-8. \\(~\\) \\(~\\) addition, can gain visual insight adjustments made functions plotting adjusted absolute values along naive estimates true absolute \\(\\beta\\) values SNPs originally deemed significant, .e. raw \\(p\\)-values less 5e-8, follows:  \\(~\\) \\(~\\)","code":"sq_diff_1 <- data.frame(naive = sum((true_beta[out_CL$rsid] - out_CL$beta)^2), beta.cl1 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl1)^2),  beta.cl2 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl2)^2),  beta.cl3 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl3)^2)) sq_diff_1 #>        naive   beta.cl1    beta.cl2   beta.cl3 #> 1 0.01315265 0.01096133 0.004872073 0.00726706  mean_abs_diff_1 <- data.frame(naive = mean(abs(true_beta[out_CL$rsid] - out_CL$beta)), beta.cl1 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl1)),  beta.cl2 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl2)),  beta.cl3 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl3))) mean_abs_diff_1 #>        naive   beta.cl1    beta.cl2   beta.cl3 #> 1 0.01655319 0.01495558 0.009610264 0.01212566  frac_less_bias_1 <- data.frame(beta.cl1 = (sum(abs(true_beta[out_CL$rsid] - out_CL$beta) > abs(true_beta[out_CL$rsid] - out_CL$beta.cl1)))/nrow(out_CL), beta.cl2 = (sum(abs(true_beta[out_CL$rsid] - out_CL$beta) > abs(true_beta[out_CL$rsid] - out_CL$beta.cl2)))/nrow(out_CL), beta.cl3 = (sum(abs(true_beta[out_CL$rsid] - out_CL$beta) > abs(true_beta[out_CL$rsid] - out_CL$beta.cl3)))/nrow(out_CL)) frac_less_bias_1 #>    beta.cl1  beta.cl2  beta.cl3 #> 1 0.6176471 0.8235294 0.7352941 sq_diff_2 <- data.frame(naive = sum((true_beta[out_EB$rsid] - out_EB$beta)^2), beta_EB = sum((true_beta[out_EB$rsid] - out_EB$beta_EB)^2), beta_FIQT = sum((true_beta[out_FIQT$rsid] - out_FIQT$beta_FIQT)^2), beta_BR_ss = sum((true_beta[out_BR_ss$rsid] - out_BR_ss$beta_BR_ss)^2)) sq_diff_2 #>      naive  beta_EB beta_FIQT beta_BR_ss #> 1 155.8238 1.840651  1.867272   15.13993  mean_abs_diff_2 <- data.frame(naive = mean(abs(true_beta[out_EB$rsid] - out_EB$beta)), beta_EB = mean(abs(true_beta[out_EB$rsid] - out_EB$beta_EB)), beta_FIQT = mean(abs(true_beta[out_FIQT$rsid] - out_FIQT$beta_FIQT)), beta_BR_ss = mean(abs(true_beta[out_BR_ss$rsid] - out_BR_ss$beta_BR_ss))) mean_abs_diff_2 #>         naive      beta_EB    beta_FIQT  beta_BR_ss #> 1 0.009104013 0.0002067548 0.0002557587 0.002778236  frac_less_bias_2 <- data.frame(beta_EB = (sum(abs(true_beta[out_EB$rsid] - out_EB$beta) > abs(true_beta[out_EB$rsid] - out_EB$beta_EB)))/nrow(out_EB), beta_FIQT = (sum(abs(true_beta[out_FIQT$rsid] - out_FIQT$beta) > abs(true_beta[out_FIQT$rsid] - out_FIQT$beta_FIQT)))/nrow(out_FIQT), beta_BR_ss = (sum(abs(true_beta[out_BR_ss$rsid] - out_BR_ss$beta) > abs(true_beta[out_BR_ss$rsid] - out_BR_ss$beta_BR_ss)))/nrow(out_BR_ss)) frac_less_bias_2 #>    beta_EB beta_FIQT beta_BR_ss #> 1 0.991785  0.994367   0.994128 out_EB_sig <- out_EB[2*(1-pnorm(abs(out_EB$beta/out_EB$se))) < 5e-8,] out_FIQT_sig <- out_FIQT[2*(1-pnorm(abs(out_FIQT$beta/out_FIQT$se))) < 5e-8,] out_BR_ss_sig <- out_BR_ss[2*(1-pnorm(abs(out_BR_ss$beta/out_BR_ss$se))) < 5e-8,]   sq_diff_3 <- data.frame(naive = sum((true_beta[out_CL$rsid] - out_CL$beta)^2), beta.cl1 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl1)^2),  beta.cl2 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl2)^2),  beta.cl3 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl3)^2), beta_EB = sum((true_beta[out_EB_sig$rsid] - out_EB_sig$beta_EB)^2), beta_FIQT = sum((true_beta[out_FIQT_sig$rsid] - out_FIQT_sig$beta_FIQT)^2), beta_BR_ss = sum((true_beta[out_BR_ss_sig$rsid] - out_BR_ss_sig$beta_BR_ss)^2)) sq_diff_3 #>        naive   beta.cl1    beta.cl2   beta.cl3     beta_EB   beta_FIQT #> 1 0.01315265 0.01096133 0.004872073 0.00726706 0.005337383 0.003633531 #>    beta_BR_ss #> 1 0.003402189  mean_abs_diff_3 <- data.frame(naive = mean(abs(true_beta[out_CL$rsid] - out_CL$beta)), beta.cl1 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl1)),  beta.cl2 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl2)),  beta.cl3 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl3)), beta_EB = mean(abs(true_beta[out_EB_sig$rsid] - out_EB_sig$beta_EB)), beta_FIQT = mean(abs(true_beta[out_FIQT_sig$rsid] - out_FIQT_sig$beta_FIQT)), beta_BR_ss = mean(abs(true_beta[out_BR_ss_sig$rsid] - out_BR_ss_sig$beta_BR_ss))) mean_abs_diff_3 #>        naive   beta.cl1    beta.cl2   beta.cl3     beta_EB   beta_FIQT #> 1 0.01655319 0.01495558 0.009610264 0.01212566 0.009245785 0.007019747 #>    beta_BR_ss #> 1 0.006709485  frac_less_bias_3 <- data.frame(beta.cl1 = (sum(abs(true_beta[out_CL$rsid] - out_CL$beta) > abs(true_beta[out_CL$rsid] - out_CL$beta.cl1)))/nrow(out_CL), beta.cl2 = (sum(abs(true_beta[out_CL$rsid] - out_CL$beta) > abs(true_beta[out_CL$rsid] - out_CL$beta.cl2)))/nrow(out_CL), beta.cl3 = (sum(abs(true_beta[out_CL$rsid] - out_CL$beta) > abs(true_beta[out_CL$rsid] - out_CL$beta.cl3)))/nrow(out_CL), beta_EB = (sum(abs(true_beta[out_EB_sig$rsid] - out_EB_sig$beta) > abs(true_beta[out_EB_sig$rsid] - out_EB_sig$beta_EB)))/nrow(out_EB_sig), beta_FIQT = (sum(abs(true_beta[out_FIQT_sig$rsid] - out_FIQT_sig$beta) > abs(true_beta[out_FIQT_sig$rsid] - out_FIQT_sig$beta_FIQT)))/nrow(out_FIQT_sig), beta_BR_ss = (sum(abs(true_beta[out_BR_ss_sig$rsid] - out_BR_ss_sig$beta) > abs(true_beta[out_BR_ss_sig$rsid] - out_BR_ss_sig$beta_BR_ss)))/nrow(out_BR_ss_sig)) frac_less_bias_3 #>    beta.cl1  beta.cl2  beta.cl3   beta_EB beta_FIQT beta_BR_ss #> 1 0.6176471 0.8235294 0.7352941 0.8823529 0.8529412  0.8529412 library(\"RColorBrewer\") col <- brewer.pal(8,\"Dark2\")    plot(density(abs(out_EB_sig$beta)),ylim=c(0,80),xlim=c(-0.01,0.09),main=\"Comparing Winner's Curse Methods\",col=col[1],lwd=2) lines(density(abs(true_beta[out_CL$rsid])),col=col[2],lwd=2) lines(density(abs(out_FIQT_sig$beta_FIQT)),col=col[3],lwd=2) lines(density(abs(out_CL$beta.cl1)),col=col[4],lwd=2) lines(density(abs(out_CL$beta.cl2)),col=col[5],lwd=2) lines(density(abs(out_CL$beta.cl3)),col=col[6],lwd=2) lines(density(abs(out_EB_sig$beta_EB)),col=col[7],lwd=2) lines(density(abs(out_BR_ss_sig$beta_BR_ss)),col=col[8],lwd=2) legend(-0.008, 77.5, legend=c(\"beta_naive\", \"true_beta\",\"beta_FIQT\",\"beta.cl1\",\"beta.cl2\",\"beta.cl3\",\"beta_EB\", \"beta_BR_ss\"),        col=col,lty=1,lwd=2)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Amanda Forde. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Forde (2022). winnerscurse: Winner's Curse Adjustment Methods GWAS Summary Statistics. R package version 0.1.1, https://amandaforde.github.io/winnerscurse/.","code":"@Manual{,   title = {winnerscurse: Winner's Curse Adjustment Methods for GWAS Summary Statistics},   author = {Amanda Forde},   year = {2022},   note = {R package version 0.1.1},   url = {https://amandaforde.github.io/winnerscurse/}, }"},{"path":"/index.html","id":"winners-curse-adjustment-methods-for-gwas-summary-statistics","dir":"","previous_headings":"","what":"Winner's Curse Adjustment Methods for GWAS Summary Statistics","title":"Winner's Curse Adjustment Methods for GWAS Summary Statistics","text":"package, winnerscurse, designed provide easy access published methods aim correct Winner’s Curse, using GWAS summary statistics. merely estimates SNP-trait association, beta, corresponding standard error, se, SNP, package permits users implement adjustment methods obtain less biased estimates true beta values. Methods can applied data relating quantitative binary traits. package contains functions can implemented just summary statistics discovery GWAS well functions require summary data discovery replication GWASs. Users can also obtain confidence intervals standard errors certain adjusted association estimates.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Winner's Curse Adjustment Methods for GWAS Summary Statistics","text":"can install current version winnerscurse GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"amandaforde/winnerscurse\")"},{"path":"/index.html","id":"winners-curse","dir":"","previous_headings":"","what":"Winner’s Curse","title":"Winner's Curse Adjustment Methods for GWAS Summary Statistics","text":"Winner’s Curse statistical effect usually resulting exaggeration SNP-trait association estimates sample associations discovered. However, Winner’s Curse isn’t just phenomenon related GWAS. understanding Winner’s Curse may gained following simple example. Consider rugby players ranked based number points scored one World Cup tournament. top ranking players, ‘winners’, likely players average tournament, perhaps peak career fitness. However, across many tournaments, highly ranked players may score many points outstanding consistently. Now, let us reframe idea context GWAS. ‘winners’ SNPs whose effect sizes stochastically higher discovery study true association values. Clearly, raw effect estimates therefore biased estimates beta, especially SNPs ranked highly study - significant SNPs. SNPs often ranked according z-statistics corresponding p-values. goal functions package adjust raw effect estimates, beta, rendering less biased. adjustments made using summary statistics obtained discovery study.","code":""},{"path":"/index.html","id":"simple-example","dir":"","previous_headings":"","what":"Simple Example","title":"Winner's Curse Adjustment Methods for GWAS Summary Statistics","text":"now discuss simple simulated data set, providing another way comprehending Winner’s Curse assistance plot. However, remember throughout much hypothetical data set used demonstration purposes resemble true data set one obtain GWAS. Let us first generate 1000 values standard normal distribution, centred 0 standard deviation 1. allow values act true effect sizes 1000 independent SNPs. Next, use true effect sizes simulate one effect estimate SNP - similar performing single GWAS study looking points scored players one World Cup tournament. imagine effect size SNP also follows normal distribution centred true effect size, beta standard deviation 0.5. value 0.5 used convenience general, SNP individual value . Therefore, beta_hat can understood vector estimates effect sizes obtained one study. re-arrange data set SNPs now ordered based decreasing absolute beta_hat values look 6 extreme SNPs. data set simulated, know true beta values thus, can easily compare beta_hat beta SNP. real data sets, fortunate. table , bias induced Winner’s Curse evident. beta_hat value overestimation corresponding true effect size.  Plotting distribution true absolute effect sizes SNPs ranked top 50 together estimated absolute effect sizes study allows us visualize Winner’s Curse. expected, witness orange curve estimated effects lying right true effect turquoise curve. provides strong indication large number estimated effects extreme SNPs greater corresponding true value.  Therefore, visual terms, like functions package produce estimates line true values, .e. shifted towards turquoise density plot left, avoiding obvious inflation incurred raw beta_hat estimates . Note: data suitable use functions package. merely used simple example demonstrate concept winner’s curse. order appropriately use functions, summary statistics must form data frame first column, titled rsid, contains SNP ID number, second column, named beta, contains effect size estimate third column, se holds corresponding estimated standard error.","code":"set.seed(1948) beta <- stats::rnorm(1000,0,1) beta <- data.frame(id = 1:1000, beta = beta) beta_hat <- stats::rnorm(1000,beta$beta,0.5)   data <- cbind(beta, beta_hat) data <- dplyr::arrange(data, desc(abs(beta_hat))) head(data) library(RColorBrewer) col <- brewer.pal(3,\"Dark2\") plot(density(abs(data$beta[1:50])),ylim=c(0,1.4),xlim=c(0.5,4.5), main=\"Visualisation of Winner's Curse\", col=col[1], lwd=2) lines(density(abs(data$beta_hat[1:50])), col=col[2], lwd=2)"},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Amanda Forde Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/BR_ss.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap method for use with discovery GWAS — BR_ss","title":"Bootstrap method for use with discovery GWAS — BR_ss","text":"BR_ss function aims use summary statistics alleviate Winner's Curse bias SNP-trait association estimates, obtained discovery GWAS. function implements adaptation bootstrap resampling method known BR-squared, detailed Faye et al. (2011).","code":""},{"path":"/reference/BR_ss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap method for use with discovery GWAS — BR_ss","text":"","code":"BR_ss(summary_data, seed_opt = FALSE, seed = 1998)"},{"path":"/reference/BR_ss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap method for use with discovery GWAS — BR_ss","text":"summary_data data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. function requires must least 5 SNPs less result issues upon usage smoothing spline. seed_opt logical value allows user choose wish set seed, order ensure reproducibility adjusted estimates. Small differences can occur iterations function data set due use parametric bootstrapping. default setting seed_opt=FALSE. seed numerical value specifies seed used seed_opt=TRUE. default setting arbitrary value 1998.","code":""},{"path":"/reference/BR_ss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap method for use with discovery GWAS — BR_ss","text":"data frame inputted summary data occupying first three  columns. new adjusted association estimates SNP returned  fourth column, namely beta_BR_ss. SNPs contained  data frame according significance, significant SNP,  .e. SNP largest absolute \\(z\\)-statistic, now located  first row data frame.","code":""},{"path":"/reference/BR_ss.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bootstrap method for use with discovery GWAS — BR_ss","text":"Faye, L. L., Sun, L., Dimitromanolakis, ., & Bull, S. B. (2011).  flexible genome-wide bootstrap method accounts ranking  threshold-selection bias GWAS interpretation replication study  design. Statistics Medicine, 30(15), 1898\\(-\\)1912.  https://doi.org/10.1002/sim.4228","code":""},{"path":[]},{"path":"/reference/cl_interval.html","id":null,"dir":"Reference","previous_headings":"","what":"Confidence interval associated with discovery GWAS conditional likelihood\r\nmethods — cl_interval","title":"Confidence interval associated with discovery GWAS conditional likelihood\r\nmethods — cl_interval","text":"cl_interval function allows user obtain confidence interval adjusted association estimates significant SNPs, obtained implementation conditional_likelihood. function produces one confidence interval significant SNP, based approach suggested Ghosh et al. (2008). Note order appropriate confidence interval outputted significant SNP, absolute value largest \\(z\\)-statistic data set must less 150.","code":""},{"path":"/reference/cl_interval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confidence interval associated with discovery GWAS conditional likelihood\r\nmethods — cl_interval","text":"","code":"cl_interval(summary_data, alpha = 5e-08, conf_level = 0.95)"},{"path":"/reference/cl_interval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confidence interval associated with discovery GWAS conditional likelihood\r\nmethods — cl_interval","text":"summary_data data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. alpha numerical value specifies desired genome-wide significance threshold. default given 5e-8. conf_level numerical value 0 1 determines confidence interval computed. default setting 0.95 results calculation 95% confidence interval adjusted association estimate SNP.","code":""},{"path":"/reference/cl_interval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confidence interval associated with discovery GWAS conditional likelihood\r\nmethods — cl_interval","text":"data frame combines output conditional_likelihood two additional columns, namely lower upper, containing lower upper bounds   required confidence interval significant SNP, respectively.","code":""},{"path":"/reference/cl_interval.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Confidence interval associated with discovery GWAS conditional likelihood\r\nmethods — cl_interval","text":"Ghosh, ., Zou, F., & Wright, F. . (2008). Estimating odds   ratios genome scans: approximate conditional likelihood approach.   American journal human genetics, 82(5), 1064\\(-\\)1074.   https://doi.org/10.1016/j.ajhg.2008.03.002","code":""},{"path":[]},{"path":"/reference/conditional_likelihood.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional likelihood methods for use with discovery GWAS — conditional_likelihood","title":"Conditional likelihood methods for use with discovery GWAS — conditional_likelihood","text":"conditional_likelihood function uses summary statistics correct bias created Winner's Curse phenomenon SNP-trait association estimates, obtained discovery GWAS, SNPs considered significant. function implements approximate conditional likelihood approach, discussed Ghosh et al. (2008), suggests three different forms less biased association estimate. Note order appropriate value second form adjusted estimate, namely beta.cl2, outputted significant SNP, absolute value largest \\(z\\)-statistic data set must less 150.","code":""},{"path":"/reference/conditional_likelihood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional likelihood methods for use with discovery GWAS — conditional_likelihood","text":"","code":"conditional_likelihood(summary_data, alpha = 5e-08)"},{"path":"/reference/conditional_likelihood.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional likelihood methods for use with discovery GWAS — conditional_likelihood","text":"summary_data data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. alpha numerical value specifies desired genome-wide significance threshold. default given 5e-8.","code":""},{"path":"/reference/conditional_likelihood.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional likelihood methods for use with discovery GWAS — conditional_likelihood","text":"data frame summary statistics adjusted association   estimates SNPs deemed significant according   specified threshold, alpha, .e. SNPs \\(p\\)-values   less alpha. inputted summary data occupies first three   columns. new adjusted association estimates SNP, defined   aforementioned paper, contained next three columns, namely beta.cl1, beta.cl2 beta.cl3. SNPs   contained data frame according significance,   significant SNP, .e. SNP largest absolute \\(z\\)-statistic,   now located first row data frame. SNPs detected   significant data set, conditional_likelihood merely returns   inputted data frame, summary_data.","code":""},{"path":"/reference/conditional_likelihood.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conditional likelihood methods for use with discovery GWAS — conditional_likelihood","text":"Ghosh, ., Zou, F., & Wright, F. . (2008). Estimating odds   ratios genome scans: approximate conditional likelihood approach.   American journal human genetics, 82(5), 1064\\(-\\)1074.   https://doi.org/10.1016/j.ajhg.2008.03.002","code":""},{"path":[]},{"path":"/reference/condlike_rep.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional likelihood method for use with discovery and replication GWASs — condlike_rep","title":"Conditional likelihood method for use with discovery and replication GWASs — condlike_rep","text":"condlike_rep function attempts produce less biased SNP-trait association estimates SNPs deemed significant discovery GWAS, using summary statistics discovery replication GWASs. function computes three new association estimates SNP manner based closely method described Zhong Prentice (2008). also returns confidence intervals new association estimate, desired user.","code":""},{"path":"/reference/condlike_rep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional likelihood method for use with discovery and replication GWASs — condlike_rep","text":"","code":"condlike_rep(   summary_disc,   summary_rep,   alpha = 5e-08,   conf_interval = FALSE,   conf_level = 0.95 )"},{"path":"/reference/condlike_rep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional likelihood method for use with discovery and replication GWASs — condlike_rep","text":"summary_disc data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. summary_rep data frame containing summary statistics replication GWAS. must three columns column names rsid, beta se, respectively, columns must contain numerical values. row must correspond unique SNP, identified numerical value rsid. SNPs must ordered exact manner summary_disc, .e. summary_rep$rsid must equivalent summary_disc$rsid. alpha numerical value specifies desired genome-wide significance threshold discovery GWAS. default given 5e-8. conf_interval logical value determines whether confidence intervals form adjusted association estimate also computed outputted. default conf_interval=FALSE. conf_level numerical value 0 1 specifies confidence interval computed. default setting 0.95 results calculation 95% confidence interval adjusted association estimate SNP.","code":""},{"path":"/reference/condlike_rep.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conditional likelihood method for use with discovery and replication GWASs — condlike_rep","text":"data frame summary statistics adjusted association   estimates SNPs deemed significant   discovery GWAS according specified threshold, alpha, .e.   SNPs \\(p\\)-values less alpha. inputted summary data   occupies first five columns, columns beta_disc se_disc contain statistics discovery GWAS columns beta_rep se_rep hold replication GWAS statistics.   default setting conf_interval=FALSE, new adjusted   association estimates SNP, defined aforementioned paper,   contained next three columns, namely beta_com, beta_MLE beta_MSE. case conf_interval=TRUE, lower upper boundaries   confidence interval form adjusted estimate included   data frame well adjusted estimates SNP. SNPs   contained data frame according significance,   significant SNP, .e. SNP largest absolute \\(z\\)-statistic,   now located first row data frame. SNPs detected   significant discovery GWAS, condlike_rep merely returns   data frame combines two inputted data sets.","code":""},{"path":"/reference/condlike_rep.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conditional likelihood method for use with discovery and replication GWASs — condlike_rep","text":"Zhong, H., & Prentice, R. L. (2008). Bias-reduced estimators   confidence intervals odds ratios genome-wide association studies.   Biostatistics (Oxford, England), 9(4), 621\\(-\\)634.   https://doi.org/10.1093/biostatistics/kxn001","code":""},{"path":[]},{"path":"/reference/empirical_bayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Empirical Bayes method for use with discovery GWAS — empirical_bayes","title":"Empirical Bayes method for use with discovery GWAS — empirical_bayes","text":"empirical_bayes function uses summary statistics correct bias induced Winner's Curse SNP-trait association estimates, obtained discovery GWAS. function strongly based method detailed Ferguson et al. (2013).","code":""},{"path":"/reference/empirical_bayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Empirical Bayes method for use with discovery GWAS — empirical_bayes","text":"","code":"empirical_bayes(summary_data, AIC = TRUE)"},{"path":"/reference/empirical_bayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Empirical Bayes method for use with discovery GWAS — empirical_bayes","text":"summary_data data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. function requires must least 50 SNPs less result poor performance method. AIC logical value allows user choose wish use AIC approach determine appropriate value degrees freedom. default setting AIC=TRUE. AIC=FALSE, degrees freedom set 6.","code":""},{"path":"/reference/empirical_bayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Empirical Bayes method for use with discovery GWAS — empirical_bayes","text":"data frame inputted summary data occupying first three  columns. new adjusted association estimates SNP returned  fourth column, namely beta_EB. SNPs contained  data frame according significance, significant SNP,  .e. SNP largest absolute \\(z\\)-statistic, now located  first row data frame.","code":""},{"path":"/reference/empirical_bayes.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Empirical Bayes method for use with discovery GWAS — empirical_bayes","text":"Ferguson, J. P., Cho, J. H., Yang, C., & Zhao, H. (2013).  Empirical Bayes correction Winner's Curse genetic association  studies. Genetic epidemiology, 37(1), 60\\(-\\)68.  https://doi.org/10.1002/gepi.21683","code":""},{"path":[]},{"path":"/reference/FDR_IQT.html","id":null,"dir":"Reference","previous_headings":"","what":"FDR IQT method for use with discovery GWAS — FDR_IQT","title":"FDR IQT method for use with discovery GWAS — FDR_IQT","text":"FDR_IQT function uses summary statistics reduce Winner's Curse bias SNP-trait association estimates, obtained discovery GWAS. function implements FDR Inverse Quantile Transformation method described Bigdeli et al. (2016), established purpose.","code":""},{"path":"/reference/FDR_IQT.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FDR IQT method for use with discovery GWAS — FDR_IQT","text":"","code":"FDR_IQT(summary_data, min_pval = 1e-15)"},{"path":"/reference/FDR_IQT.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"FDR IQT method for use with discovery GWAS — FDR_IQT","text":"summary_data data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified numerical value rsid. min_pval numerical value whose purpose avoid zero \\(p\\)-values introduces issues qnorm() applied. SNP computed \\(p\\)-value found less min_pval merely re-assigned min_pval \\(p\\)-value method proceeds. definition, method makes adjustment association estimate SNP occurred presumption general, estimates SNPs \\(z > 8\\) biased. default value min_pval = 1e-15.","code":""},{"path":"/reference/FDR_IQT.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"FDR IQT method for use with discovery GWAS — FDR_IQT","text":"data frame inputted summary data occupying first three  columns. new adjusted association estimates SNP returned  fourth column, namely beta_FIQT. SNPs contained  data frame according significance, significant SNP,  .e. SNP largest absolute \\(z\\)-statistic, now located  first row data frame.","code":""},{"path":"/reference/FDR_IQT.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"FDR IQT method for use with discovery GWAS — FDR_IQT","text":"Bigdeli, T. B., Lee, D., Webb, B. T., Riley, B. P., Vladimirov, V.  ., Fanous, . H., Kendler, K. S., & Bacanu, S. . (2016). simple yet  accurate correction winner's curse can predict signals discovered  much larger genome scans. Bioinformatics (Oxford, England),  32(17), 2598\\(-\\)2603.  https://doi.org/10.1093/bioinformatics/btw303","code":""},{"path":[]},{"path":"/reference/MSE_minimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"MSE minimization method for use with discovery and replication GWASs — MSE_minimizer","title":"MSE minimization method for use with discovery and replication GWASs — MSE_minimizer","text":"MSE_minimizer function implements approach combines association estimates obtained discovery replication GWASs form new combined estimate SNP. method used function inspired detailed Ferguson et al. (2017).","code":""},{"path":"/reference/MSE_minimizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MSE minimization method for use with discovery and replication GWASs — MSE_minimizer","text":"","code":"MSE_minimizer(summary_disc, summary_rep, alpha = 5e-08, spline = TRUE)"},{"path":"/reference/MSE_minimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MSE minimization method for use with discovery and replication GWASs — MSE_minimizer","text":"summary_disc data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. function requires must least 5 SNPs less result issues upon usage smoothing spline. summary_rep data frame containing summary statistics replication GWAS. must three columns column names rsid, beta se, respectively, columns must contain numerical values. row must correspond unique SNP, identified numerical value rsid. SNPs must ordered exact manner summary_disc, .e. summary_rep$rsid must equivalent summary_disc$rsid. alpha numerical value specifies desired genome-wide significance threshold discovery GWAS. default given 5e-8. spline logical value determines whether cubic smoothing spline used. spline=FALSE, value \\(B\\) formula detailed aforementioned paper merely calculated B=summary_disc$beta - summary_rep$beta SNP. Alternatively, spline=TRUE applies cubic smoothing spline predict values \\(B\\) B=summary_disc$beta - summary_rep$beta regressed z=summary_disc$beta/summary_disc$se, predicted values used \\(B\\).","code":""},{"path":"/reference/MSE_minimizer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MSE minimization method for use with discovery and replication GWASs — MSE_minimizer","text":"data frame summary statistics adjusted association   estimate SNPs deemed significant   discovery GWAS according specified threshold, alpha, .e.   SNPs \\(p\\)-values less alpha. inputted summary data   occupies first five columns, columns beta_disc se_disc contain statistics discovery GWAS columns beta_rep se_rep hold replication GWAS statistics.   new combination estimate SNPis contained final column,   namely beta_joint. SNPs contained data frame   according significance, significant SNP, .e.   SNP largest absolute \\(z\\)-statistic, now located first   row data frame. SNPs detected significant   discovery GWAS, MSE_minimizer merely returns data frame   combines two inputted data sets.","code":""},{"path":"/reference/MSE_minimizer.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"MSE minimization method for use with discovery and replication GWASs — MSE_minimizer","text":"Ferguson, J., Alvarez-Iglesias, ., Newell, J., Hinde, J., &   O'Donnell, M. (2017).  Joint incorporation randomised observational   evidence estimating treatment effects. Statistical Methods   Medical Research, 28(1), 235\\(-\\)247.   https://doi.org/10.1177/0962280217720854","code":""},{"path":[]},{"path":"/reference/se_adjust.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard errors of adjusted discovery GWAS estimates via parametric bootstrap — se_adjust","title":"Standard errors of adjusted discovery GWAS estimates via parametric bootstrap — se_adjust","text":"se_adjust function allows user obtain approximate standard errors adjusted association estimates, means parametric bootstrapping. Standard errors can evaluated estimates corrected Empirical Bayes method, FDR Inverse Quantile Transformation method bootstrap method. Note comparison functions package, function can computationally intensive take several minutes run, depending size data set, method number bootstraps chosen.","code":""},{"path":"/reference/se_adjust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard errors of adjusted discovery GWAS estimates via parametric bootstrap — se_adjust","text":"","code":"se_adjust(summary_data, method, n_boot = 100)"},{"path":"/reference/se_adjust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standard errors of adjusted discovery GWAS estimates via parametric bootstrap — se_adjust","text":"summary_data data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. method string specifying function implemented bootstrap samples. take form \"BR_ss\", \"empirical_bayes\" \"FDR_IQT\". n_boot numerical value determines number bootstrap repetitions used. must greater 1. default value 100.","code":""},{"path":"/reference/se_adjust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standard errors of adjusted discovery GWAS estimates via parametric bootstrap — se_adjust","text":"data frame combines output chosen method   additional column, namely adj_se. column provides standard   errors adjusted association estimates SNP.","code":""},{"path":[]},{"path":"/reference/UMVCUE.html","id":null,"dir":"Reference","previous_headings":"","what":"UMVCUE method for use with discovery and replication GWASs — UMVCUE","title":"UMVCUE method for use with discovery and replication GWASs — UMVCUE","text":"UMVCUE function aims produce less biased SNP-trait association estimates SNPs deemed significant discovery GWAS, using summary statistics discovery replication GWASs. function implements method described Bowden Dudbridge (2009), established purpose.","code":""},{"path":"/reference/UMVCUE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"UMVCUE method for use with discovery and replication GWASs — UMVCUE","text":"","code":"UMVCUE(summary_disc, summary_rep, alpha = 5e-08)"},{"path":"/reference/UMVCUE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"UMVCUE method for use with discovery and replication GWASs — UMVCUE","text":"summary_disc data frame containing summary statistics discovery GWAS. must three columns column names rsid, beta se, respectively, columns beta se must contain numerical values. row must correspond unique SNP, identified rsid. summary_rep data frame containing summary statistics replication GWAS. must three columns column names rsid, beta se, respectively, columns must contain numerical values. row must correspond unique SNP, identified numerical value rsid. SNPs must ordered exact manner summary_disc, .e. summary_rep$rsid must equivalent summary_disc$rsid. alpha numerical value specifies desired genome-wide significance threshold discovery GWAS. default given 5e-8.","code":""},{"path":"/reference/UMVCUE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"UMVCUE method for use with discovery and replication GWASs — UMVCUE","text":"data frame summary statistics adjusted association estimate  SNPs deemed significant discovery GWAS  according specified threshold, alpha, .e. SNPs \\(p\\)-values less alpha. inputted summary data occupies  first five columns, columns beta_disc se_disc contain statistics discovery GWAS columns beta_rep se_rep hold replication GWAS statistics.  new adjusted association estimate SNP, defined  aforementioned paper, contained final column, namely beta_UMVCUE. SNPs contained data frame according  significance, significant SNP, .e. SNP  largest absolute \\(z\\)-statistic, now located first row data  frame. SNPs detected significant discovery GWAS, UMVCUE merely returns data frame combines two inputted  data sets.","code":""},{"path":"/reference/UMVCUE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"UMVCUE method for use with discovery and replication GWASs — UMVCUE","text":"Bowden, J., & Dudbridge, F. (2009). Unbiased estimation odds  ratios: combining genomewide association scans replication studies.  Genetic epidemiology, 33(5), 406\\(-\\)418.  https://doi.org/10.1002/gepi.20394","code":""},{"path":[]}]
