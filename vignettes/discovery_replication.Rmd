---
title: "Methods for use with discovery and replication GWASs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Methods for use with discovery and replication GWASs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(winnerscurse)
```


This package also includes methods which use both a discovery data set and a replication data set in order to obtain a less biased estimate of $\beta$. Similar to the demonstration of methods which use just the discovery data set, we will first create two toy data sets and then, illustrate how a user may employ the package's functions to adjust for the bias imposed by winner's curse. The methods that are currently accessible include: 

1. Conditional Likelihood method - adapted from [Zhong and Prentice (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2536726/)
2. UMVCUE method - [Bowden and Dudbridge (2009)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2726957/)

It is important to note that with both of these methods, adjustments are only made to SNPs that are deemed significant in the discovery data set, i.e. these SNPs have a $p$-value less than that of the specified threshold, $\alpha$. 

A third method has been added which obtains a new association estimate for each significant SNP in the discovery data set using a combination of the discovery and replication estimates:

3. MSE minimization method - based on [Ferguson et al. (2017)](https://journals.sagepub.com/doi/full/10.1177/0962280217720854)

### Addition of toy replication data set 

```{r}
set.seed(1998)
n_snps <- 10^6
effect_snps <- 0.01*n_snps
n_samples <- 30000
maf <- runif(n_snps,0.01,0.5)
true_beta <- rnorm(effect_snps,0,1)
h2 <- 0.7 # variance explained by effect SNPs
var_y <- sum(2*maf[1:effect_snps]*(1-maf[1:effect_snps])*true_beta^2)/h2
true_beta <- true_beta/sqrt(var_y) # scaling to represent a phenotype with variance 1
true_beta <- c(true_beta, rep(0,n_snps-effect_snps))
se <- sqrt((1 - 2*maf*(1-maf)*true_beta^2)/(2*(n_samples-2)*maf*(1-maf)))
stats_disc <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se),se=se)

n_samples_rep <- 300000
se_rep <- sqrt((1 - 2*maf*(1-maf)*true_beta^2)/(2*(n_samples_rep-2)*maf*(1-maf)))
stats_rep <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se_rep),se=se_rep)

head(stats_disc)
head(stats_rep)
```


### Method 1: Conditional Likelihood 

- The function `condlike_rep` implements a version of the conditional likelihood method for obtaining bias-reduced estimates of $\beta$ described in [Zhong and Prentice (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2536726/). 

- The function requires as inputs two independent data sets, one representing a discovery GWAS, `summary_disc`, and the other a replication study with identical SNPs, `summary_rep`, as well as a specification of the significance threshold, `alpha`, to be used. As before, the data sets must be in the form of data frames with columns `rsid`, `beta` and `se`, all columns of the data frames must contain numerical values and each row of both data frames must represent a unique SNP, identified by `rsid`.  SNPs must be in the exact same order in both data sets, i.e. the identity `summary_rep$rsid == summary_disc$rsid` must evaluate to `TRUE`. 

- Furthermore, the parameter `conf_interval` in `condlike_rep` provides the user with the option to obtain confidence intervals for each of the adjusted association estimates. The default setting is `conf_interval=FALSE`. Inserting `conf_interval=TRUE` when using the function will return three sets of lower and upper confidence interval boundaries for each SNP, each set corresponding to a particular form of adjusted estimate. The final parameter `conf_level` takes a numerical value between 0 and 1 which specifies the confidence interval, with the default being `0.95`.

- In a similar manner to other functions included in the package, `condlike_rep` returns a single data frame with SNPs reordered based on significance. However, it only contains SNPs which have been deemed significant in the discovery data set. The first 5 columns of the data frame details the inputted information; `rsid`, `beta_disc`, `se_disc`, `beta_rep`, `se_rep`. Following this, `beta_com` is the inverse variance weighted estimate which is formally defined as:$$\hat\beta_{\text{com}} = \frac{\sigma_2^2 \hat\beta_1 + \sigma_1^2 \hat\beta_2}{\sigma_1^2 + \sigma_2^2},$$
in which $\hat\beta_1$ = `beta_disc`, $\hat\beta_2$ = `beta_rep`, $\sigma_1$ = `se_disc` and $\sigma_2$ = `se_rep`.

- The method implemented here uses just one selection cut-point at the first discovery stage as opposed to that described in [Zhong and Prentice (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2536726/) in which two separate selection thresholds are used. Thus, the maximum likelihood adjusted estimator, `beta_MLE` is defined to maximize the conditional likelihood at the observed $\hat\beta_{\text{com}}$:$$\hat\beta_{\text{MLE}} = \arg \max_{\beta} \log f(\hat\beta_{\text{com}}; \beta).$$
The conditional sampling distribution, $f(x;\beta)$ is approximated by:
$$f(x;\beta) = \frac{\frac{1}{\sigma_{\text{com}}} \phi\left(\frac{x-\beta}{\sigma_{\text{com}}}\right) \cdot \left[\Phi\left(\frac{x-c\sigma_1}{\frac{\sigma_1}{\sigma_2}\sigma_{\text{com}}}\right) + \Phi\left(\frac{-x-c\sigma_1}{\frac{\sigma_1}{\sigma_2}\sigma_{\text{com}}}\right)\right]}{\Phi\left(\frac{\beta}{\sigma_1} - c\right) + \Phi\left(- \frac{\beta}{\sigma_1} - c\right)}.$$

- $c$ is the selection cut-point, i.e. all SNPs with $\mid \frac{\hat\beta_1}{\sigma_1}\mid \ge c$ are deemed as significant. The value of $c$ is easily obtained using the chosen `alpha`. In addition, $$\sigma^2_{\text{com}} = \frac{\sigma_1^2 \sigma_2^2}{\sigma_1^2 + \sigma_2^2}.$$
 
- Note that this function, $f(x;\beta)$ is slightly different from that given in the paper as only one selection cut-point is imposed here. 

- Finally, Zhong and Prectice (2008) noted that simulation studies showed that $\hat\beta_{\text{com}}$ tended to have upward bias while $\hat\beta_{\text{MLE}}$ over-corrected and therefore, a combination of the two in the following form was proposed: 
$$\hat\beta_{\text{MSE}} = \frac{\hat\sigma^2_{\text{com}}\cdot \hat\beta_{\text{com}} + (\hat\beta_{\text{com}} - \hat\beta_{\text{MLE}})^2\cdot\hat\beta_{\text{MLE}}}{\sigma^2_{\text{com}}+(\hat\beta_{\text{com}} - \hat\beta_{\text{MLE}})^2}.$$
This $\hat\beta_{\text{MSE}}$ holds the final column of the outputted data frame in the default setting. 

- The use of `condlike_rep` with our toy data sets in which `conf_interval=FALSE` is demonstrated below, with a significance threshold value of `10e-6`:
```{r}
out1 <- condlike_rep(summary_disc=stats_disc, summary_rep=stats_rep, alpha=10e-6)
head(out1)
```

$~$

**Confidence intervals:**

- Firstly, the $(1-\alpha)\%$ confidence interval for $\hat\beta_{\text{com}}$ is simply calculated as: $$\hat\beta_{\text{com}} \pm \hat\sigma_{\text{com}}Z_{1-\frac{\alpha}{2}}.$$For $\hat\beta_{\text{MLE}}$, the profile confidence limits are the intersection of the log-likelihood curve with a horizontal line $\frac{\chi^2_{1,1-\alpha}}{2}$ units below its maximum. The MSE weighting method, as described above, can then be easily applied to the upper and lower boundaries of these two confidence intervals to obtain an appropriate confidence interval for $\hat\beta_{\text{MSE}}$. This gives: $$\hat\beta_{\text{MSE};\frac{\alpha}{2}} = \hat{K}_{\frac{\alpha}{2}} \hat\beta_{\text{com};\frac{\alpha}{2}} + \left(1-\hat{K}_{\frac{\alpha}{2}}\right) \hat\beta_{\text{MLE};\frac{\alpha}{2}}$$ $$\hat\beta_{\text{MSE};1-\frac{\alpha}{2}} = \hat{K}_{1-\frac{\alpha}{2}} \hat\beta_{\text{com};1-\frac{\alpha}{2}} + \left(1-\hat{K}_{1-\frac{\alpha}{2}}\right) \hat\beta_{\text{MLE};1-\frac{\alpha}{2}}$$ in which $\hat{K}_{\frac{\alpha}{2}} = \frac{\hat\sigma^2_{\text{com}}}{\hat\sigma^2_{\text{com}} + \left(\hat\beta_{\text{com};\frac{\alpha}{2}} - \hat\beta_{\text{MLE};\frac{\alpha}{2}}\right)^2} \;$ and $\;  \hat{K}_{1-\frac{\alpha}{2}} = \frac{\hat\sigma^2_{\text{com}}}{\hat\sigma^2_{\text{com}} + \left(\hat\beta_{\text{com};1-\frac{\alpha}{2}} - \hat\beta_{\text{MLE};1-\frac{\alpha}{2}}\right)^2}.$

- We implement `condlike_rep` on our toy data sets with `conf_interval` now set to `TRUE` to show the form in which the output now takes. A similar data frame to that above is returned with 95% confidence intervals also included for each adjusted association estimate for each SNP.

```{r}
out1_conf <- condlike_rep(summary_disc=stats_disc, summary_rep=stats_rep, alpha=10e-6, conf_interval=TRUE, conf_level=0.95)
head(out1_conf)
```

- **Note:** The current computation of confidence intervals may not work in every situation and thus, ongoing work is being done to resolve this. 

### Method 2: UMVCUE

- The implementation of `UMVCUE` is very similar to the function described above in the sense that `UMVCUE` requires the same inputs; discovery and replication data sets in the form of three-columned data frames together with a threshold value, `alpha`. Furthermore, the outputted data frame is in the same form with just one extra column providing the adjusted estimate, `beta_UMVCUE`.

- Selection also occurs here at just one stage - SNPs are deemed as significant if their $p$-values corresponding to $\mid \frac{\hat\beta_1}{\sigma_1}\mid$ are smaller than the given threshold. 

- The function `UMVCUE` executes the method detailed in [Bowden and Dudbridge (2009)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2726957/). No adaptations have been made to the method described. 

- It is worth noting that, as with all conditional likelihood methods, the method used in `condlike_rep` makes adjustments to each SNP one at a time with no information relating to other SNPs required for this adjustment. However, after ordering SNPs based on significance, for a single SNP, `UMVCUE` also uses the data of SNPs on either side of it to assist with the adjustment.

- `UMVCUE` can be applied to the toy data sets as followed, with `alpha` again specified as `10e-6`:
```{r}
out2 <- UMVCUE(summary_disc = stats_disc, summary_rep = stats_rep, alpha = 10e-6)
head(out2)
```

### Method 3: MSE minimization 

- The function `MSE_minimizer` implements a combination method which closely follows that described in [Ferguson *et al.* (2017)](https://journals.sagepub.com/doi/full/10.1177/0962280217720854). The function parameters used here; `summary_disc`, `summary_rep` and `alpha`, are precisely of the same form as those previously detailed in this vignette. In addition, `MSE_minimizer` has a logical parameter, namely `spline` which defaults as `spline=TRUE`. As a smoothing spline is used in the execution of this function, data corresponding to at least 5 SNPs is required. 

- An adjusted estimate is computed for each SNP which has been classified as significant in the discovery data set, based on the given threshold. Thus, similar to the above method, `MSE_minimizer` returns a data frame containing these significant SNPs with 6 columns in which the final column contains the new estimate, `beta_joint`.

- Following the approach detailed in [Ferguson *et al.* (2017)](https://journals.sagepub.com/doi/full/10.1177/0962280217720854), we define the adjusted linear combination estimator as: $$\hat\beta_{\text{joint}} = \omega(\hat{B}) \cdot \hat\beta_{\text{rep}} + (1-\omega(\hat{B}))\cdot \hat\beta_{\text{disc}}$$ in which $$ \omega(\hat{B}) = \frac{\frac{1}{\sigma^2_{\text{rep}}}}{\frac{1}{\sigma^2_{\text{rep}}}+\frac{1}{\sigma^2_{\text{disc}}+ \hat{B}^2}}.$$ When `spline=FALSE` is used, we simply let $\hat{B} = \hat\beta_{\text{disc}} - \hat\beta_{\text{rep}}$. We make the assumptions that $\beta_{\text{rep}}$ is unbiased for $\beta$, but $\beta_{\text{disc}}$ is quite likely to be biased and that $\beta_{\text{rep}}$ and $\beta_{\text{disc}}$ are independent. 

- For the default setting `spline=TRUE`, a cubic smoothing spline is applied in which the values of $z_{\text{disc}} = \frac{\hat\beta_{\text{disc}}}{\sigma_{\text{disc}}}$ are considered as inputs and $\hat{B} = \hat\beta_{\text{disc}} - \hat\beta_{\text{rep}}$, the corresponding outputs. The predicted values for $\hat{B}$ from this process, $\hat{B}^*$ say, are then used instead of $\hat{B}$ when computing $\hat\beta_{\text{joint}}$ for each SNP.

- We apply `MSE_minimizer` to our toy data sets, once with the default setting for `spline` and once with `spline=FALSE`. Again for convenient demonstration purposes, we specify the significance threshold as `10e-6`.
```{r}
out3 <- MSE_minimizer(summary_disc = stats_disc, summary_rep = stats_rep, alpha=10e-6, spline=FALSE)
out4 <- MSE_minimizer(summary_disc = stats_disc, summary_rep = stats_rep, alpha=10e-6)

head(out3)
head(out4)
```





### Visualisation


- We can illustrate the performance of these three functions; `condlike_rep`, `UMVCUE` and `MSE_minimizer`, as follows. It is clear that all methods show an improvement on the estimates obtained from the discovery data set. However, the plots show that a further investigation will be required in order to evaluate if the adjusted estimates are less biased than the mere use of replication estimates. 

- In the second graph, it can be seen that there must be very little difference between the replication estimates and those obtained using `UMVCUE` as the density curves of `beta_rep` and `beta_UMVCUE` nearly overlap completely. 

```{r, warning = FALSE}
library("RColorBrewer")
col <- brewer.pal(8,"Dark2")

plot(density(abs(out1$beta_disc)),ylim=c(0,68),xlim=c(-0.01,0.08),main="Conditional Likelihood Winner's Curse Adjustment",col=col[1],lwd=2)
lines(density(abs(true_beta[out1$rsid])),col=col[2],lwd=2)
lines(density(abs(out1$beta_rep)),col=col[3],lwd=2)
lines(density(abs(out1$beta_com)),col=col[4],lwd=2)
lines(density(abs(out1$beta_MLE)),col=col[5],lwd=2)
lines(density(abs(out1$beta_MSE)),col=col[8],lwd=2)
legend(0.05, 67.5, legend=c("beta_disc", "true_beta","beta_rep","beta_com", "beta_MLE", "beta_MSE"),col=c(col[1:5],col[8]),lty=1,lwd=2)

plot(density(abs(out2$beta_disc)),ylim=c(0,72),xlim=c(-0.01,0.08),main="UMVCUE and MSE-minimizer Winner's Curse Adjustment",col=col[1],lwd=2)
lines(density(abs(true_beta[out2$rsid])),col=col[2],lwd=2)
lines(density(abs(out2$beta_rep)),col=col[3],lwd=2)
lines(density(abs(out2$beta_UMVCUE)),col=col[4],lwd=2)
lines(density(abs(out3$beta_joint)),col=col[5],lwd=2)
lines(density(abs(out4$beta_joint)),col=col[6],lwd=2)
legend(0.05, 70.5, legend=c("beta_disc", "true_beta","beta_rep","beta_UMVCUE", "beta_joint", "beta_joint_sp"),
       col=col,lty=1,lwd=2)
```

- In addition, as the data sets are simulated, we obtain measures such as sum of squared differences and mean of absolute differences for the various estimates as follows: 


```{r}
sq_diff <- data.frame(disc_naive = sum((true_beta[out2$rsid] - out2$beta_disc)^2), rep_naive = sum((true_beta[out2$rsid] - out2$beta_rep)^2),  beta_UMVCUE = sum((true_beta[out2$rsid] - out2$beta_UMVCUE)^2), beta_com = sum((true_beta[out1$rsid] - out1$beta_com)^2), beta_MLE = sum((true_beta[out1$rsid] - out1$beta_MLE)^2), beta_MSE = sum((true_beta[out1$rsid] - out1$beta_MSE)^2), beta_joint = sum((true_beta[out3$rsid] - out3$beta_joint)^2), beta_joint_sp = sum((true_beta[out4$rsid] - out4$beta_joint)^2))
sq_diff

mean_abs_diff <- data.frame(disc_naive = mean(abs(true_beta[out2$rsid] - out2$beta_disc)), rep_naive = mean(abs(true_beta[out2$rsid] - out2$beta_rep)),  beta_UMVCUE = mean(abs(true_beta[out2$rsid] - out2$beta_UMVCUE)), beta_com = mean(abs(true_beta[out1$rsid] - out1$beta_com)),beta_MLE = mean(abs(true_beta[out1$rsid] - out1$beta_MLE)), beta_MSE = mean(abs(true_beta[out1$rsid] - out1$beta_MSE)), beta_joint = mean(abs(true_beta[out3$rsid] - out3$beta_joint)), beta_joint_sp = mean(abs(true_beta[out4$rsid] - out4$beta_joint)))
mean_abs_diff

frac_less_bias <- data.frame(beta_UMVCUE = (sum(abs(true_beta[out2$rsid] - out2$beta_rep) > abs(true_beta[out2$rsid] - out2$beta_UMVCUE)))/nrow(out2), beta_com = (sum(abs(true_beta[out1$rsid] - out1$beta_rep) > abs(true_beta[out1$rsid] - out1$beta_com)))/nrow(out1), beta_MLE = (sum(abs(true_beta[out1$rsid] - out1$beta_rep) > abs(true_beta[out1$rsid] - out1$beta_MLE)))/nrow(out1), beta_MSE = (sum(abs(true_beta[out1$rsid] - out1$beta_rep) > abs(true_beta[out1$rsid] - out1$beta_MSE)))/nrow(out1), beta_joint = (sum(abs(true_beta[out3$rsid] - out3$beta_rep) > abs(true_beta[out3$rsid] - out3$beta_joint)))/nrow(out3), beta_joint_sp = (sum(abs(true_beta[out4$rsid] - out4$beta_rep) > abs(true_beta[out4$rsid] - out4$beta_joint)))/nrow(out4))
frac_less_bias  # comparison with naive replication estimate
```

### Using Empirical Bayes method when a replication data set is available

- When a replication data set is also available, as has been described throughout this document, we also have the option to employ the empirical Bayes method. We have seen this work considerably well in simulations and thus advise exploration of its implementation in this setting. 

- With data in the form described above similar to `stats_disc` and `stats_rep`, implementation of the empirical Bayes method could take place on the combined estimator as shown below: 

```{r, warning=FALSE}
  com_stats <- stats_disc
  com_stats$beta <- ((((stats_rep$se)^2)*(stats_disc$beta))+(((stats_disc$se)^2)*(stats_rep$beta)))/(((stats_disc$se)^2) + ((stats_rep$se)^2))
  com_stats$se <- sqrt((((stats_disc$se)^2)*((stats_rep$se)^2))/(((stats_disc$se)^2) + ((stats_rep$se)^2)))
  out_EB_com <- empirical_bayes(com_stats)
  out_EB_com <- dplyr::arrange(out_EB_com,out_EB_com$rsid)
  out_EB_com <- data.frame(rsid = stats_disc$rsid, beta_disc = stats_disc$beta, se_disc  = stats_disc$se, beta_rep = stats_rep$beta, se_rep = stats_rep$se, beta_EB=out_EB_com$beta_EB)

  ## reorder based on significance in first set up!
  out_EB_com <- dplyr::arrange(out_EB_com, dplyr::desc(abs(out_EB_com$beta_disc/out_EB_com$se_disc)))
  out_EB_com <- out_EB_com[abs(out_EB_com$beta_disc/out_EB_com$se_disc) > stats::qnorm(1-(10e-6)/2),]
  head(out_EB_com)
```

- We then compute the evaluation metrics as considered above as well as create a plot depicting the results. 

```{r}
sq_diff <- data.frame(disc_naive = sum((true_beta[out2$rsid] - out2$beta_disc)^2), rep_naive = sum((true_beta[out2$rsid] - out2$beta_rep)^2),  beta_EB_joint = sum((true_beta[out_EB_com$rsid] - out_EB_com$beta_EB)^2))
sq_diff

mean_abs_diff <- data.frame(disc_naive = mean(abs(true_beta[out2$rsid] - out2$beta_disc)), rep_naive = mean(abs(true_beta[out2$rsid] - out2$beta_rep)),  beta_EB_joint = mean(abs(true_beta[out_EB_com$rsid] - out_EB_com$beta_EB)))
mean_abs_diff

frac_less_bias <- data.frame(beta_EB_joint = (sum(abs(true_beta[out_EB_com$rsid] - out_EB_com$beta_rep) > abs(true_beta[out_EB_com$rsid] - out_EB_com$beta_EB)))/nrow(out_EB_com))
frac_less_bias  # comparison with naive replication estimate
```


```{r}
plot(density(abs(out_EB_com$beta_disc)),ylim=c(0,72),xlim=c(-0.01,0.08),main="Empirical Bayes Winner's Curse Adjustment",col=col[1],lwd=2)
lines(density(abs(true_beta[out_EB_com$rsid])),col=col[2],lwd=2)
lines(density(abs(out_EB_com$beta_rep)),col=col[3],lwd=2)
lines(density(abs(out_EB_com$beta_EB)),col=col[4],lwd=2)
legend(0.05, 70.5, legend=c("beta_disc", "true_beta","beta_rep","beta_EB_joint"),
       col=col,lty=1,lwd=2)
```

