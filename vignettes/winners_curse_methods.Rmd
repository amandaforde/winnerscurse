---
title: "Winner's Curse Adjustment Methods for GWAS summary statistics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Winner's Curse Adjustment Methods for GWAS summary statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(winnerscurse)
```


Brief description of winner's curse bias... 
A package designed to provide easy access to published methods which aim to correct for Winner's Curse using only GWAS summary statistics. With merely estimates of beta and associated standard error for each SNP, users are able to implement these methods to obtain less biased estimates of the true beta values. These methods can be applied to data from both quantitative and binary traits.


I need to start with a section on creating a dataset, and then illustrate how a user would perform each of the methods on that dataset. First, small introduction to the problem etc., then discuss the creation of a toy simulated dataset which is appropriate for use for each of my functions: 

1. Conditional Likelihood methods - [Ghosh et al. (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/)
2. Empirical Bayes method - [Ferguson et al. (2013)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/) 
3. FDR Inverse Quantile transformation method - [Bigdeli et al. (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5013908/) 



### Creating a toy data set

- generating summary statistics for 1 million SNPs in which there exists a polygenic background of 10,000 SNPs and all others have no effects
- by specifying the total number of SNPs, the number of SNPs truly associated with the trait, the number of samples, the various minor allele frequencies and finally, the variance explained in the trait by all SNPs, we can obtain quite realistic summary statistics as shown below.
- it is worth noting that we have arranged the data here in a suitable way for each method - in the form of a data frame with columns `rsid`, `beta` and `se`. 

```{r}
set.seed(1998)

n_snps <- 10^6
effect_snps <- 10000
n_samples <- 30000

maf <- runif(n_snps,0.01,0.5)
se <- 1/sqrt(2*n_samples*maf*(1-maf))

true_beta <- rnorm(effect_snps,0,1)
h2 <- 0.7 # variance explained by effect SNPs
var_y <- sum(2*maf[1:effect_snps]*(1-maf[1:effect_snps])*true_beta^2)/h2

true_beta <- true_beta/sqrt(var_y) # scaling to represent a phenotype with variance 1
true_beta <- c(true_beta, rep(0,n_snps-effect_snps))

summary_stats <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se),se=se)

head(summary_stats)
```

### Method 1: Conditional Likelihood - Ghosh et al. (2008)

The function `conditional_likelihood` requires a data frame, in the form described above, and a value for `alpha`, the significance threshold used in the GWAS. 

A data frame is returned containing only the SNPs that have $p$-values below the inputted value for the genome-wide significance threshold. The reason for this is that the conditional likelihood methods have been designed to be only used for SNPs which are deemed significant. 

Also, it is worth noting that if no SNPs are detected as significant, the function will merely return the inputted data frame. 

Furthermore, the returned data frame has SNPs ordered based on their $p$-values from smallest to largest, or equivalently, in descending order of $\mid z \mid$ in which $z$ is the estimated value for $\beta$ divided by its standard error. 

We can implement the function on our toy data set as follows, choosing a significance threshold of `5e-8`:

```{r}

out_CL <- conditional_likelihood(summary_data = summary_stats, alpha = 5e-8) 
head(out_CL)
```

We see that three columns have been added to the right of the inputted data frame. These columns each represent a correction method suggested in [Ghosh et al. (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/). The first, `beta.cl1`, is referred to as the conditional MLE while the second, `beta.cl2`, is the mean of the normalized conditional likelihood. The third, `beta.cl3` is merely the average of the first two, known as the compromise estimator. 

Ghosh et al. (2008) noted that for instances in which the true $\beta$ value is close to zero, `beta.cl2` often has greater mean squared error than `beta.cl1` but for true $\beta$ values further from zero, `beta.cl2` performs better. Thus, it was suggested that `beta.cl3` be used to combine the strengths of these two estimators. However, this function, `conditional_likelihood`, outputs values for all three estimators in order to allow the user to make their own decision on which they believe to be the most appropriate. 


### Method 2: Empirical Bayes - Ferguson et al. (2013)

The function `empirical_bayes` implements the Empirical Bayes method for correcting for Winner's Curse, detailed in [Ferguson et al. (2013)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/). The function has one argument, `summary_data`, which is the data frame containing `rsid`, `beta` and `se` columns. It returns this data frame but with a fourth column, `beta_EB` in which the adjusted estimates of this procedure have been added. As the Empirical Bayes method makes adjustments to all SNPs, not only those that have been considered significant, this data frame contains all SNPs.  


```{r}
out_EB <- empirical_bayes(summary_data = summary_stats)
out_EB[31:37,]
```


discuss issue in the tails!! - reason for beta_EB returning non-adjusted values for top 6 significant SNPs, however for lets say the 31st-37th significant SNPs which will be closer to the centre of the distribution, we can see adjustments being made! 

**TO DO:** 

- *Implement combination approach of empirical bayes and conditional likelihood method suggested in Ferguson et al. (2013) as empirical bayes is known to perform poorly in the tails of the distribution.*


### Method 3: FDR Inverse Quantile Transformation - Bigdeli et al. (2016)

The function `FDR_IQT` implements the winner's curse adjustment method detailed in [Bigdeli et al. (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5013908/). 

Similar to the approaches above, the function requires a data frame with three columns `rsid`, `beta` and `se`. It also has an argument `min_pval`, for which the default setting is `1e-15`. This is included in order to avoid zero $p$-values which can prove problematic for the function when evaluating `qnorm()`. Furthermore, due to the nature of winner's curse, it is in fact rare that observations with $\mid z \mid > 8$ are biased. 

The function outputs a data frame similar to that inputted with an additional column containing the adjusted estimate, `beta_FIQT`, and again, the SNPs have been reordered according to their individual $z$-statistics. 

```{r}
out_FIQT <- FDR_IQT(summary_data = summary_stats)
head(out_FIQT)
```



*maybe graph density of adjustments for significant SNPs or calculate MSE or something?* 
We can gain a visual insight into the adjustments made by these functions by plotting as follows: 

```{r}

out_EB_sig <- out_EB[2*(1-pnorm(abs(out_EB$beta/out_EB$se))) < 5e-8,]
out_FIQT_sig <- out_FIQT[2*(1-pnorm(abs(out_FIQT$beta/out_FIQT$se))) < 5e-8,]
  
plot(density(abs(out_EB_sig$beta)),ylim=c(0,120),xlim=c(-0.01,0.09),main="Comparing Winner's Curse Methods")
lines(density(abs(out_EB_sig$beta_EB)),col="red")
lines(density(abs(out_FIQT_sig$beta_FIQT)),col="green")
lines(density(abs(out_CL$beta.cl1)),col="blue")
lines(density(abs(out_CL$beta.cl2)),col="orange")
lines(density(abs(out_CL$beta.cl3)),col="purple")
lines(density(abs(true_beta[out_CL$rsid])),col="pink")

```
Tails issue for EB evident here: so let's have a look at EB on its own but in which we include SNPs with $p$-values less than `5e-4`: 

```{r}
out_EB_sig_4 <- out_EB[2*(1-pnorm(abs(out_EB$beta/out_EB$se))) < 5e-4,]
plot(density(abs(out_EB_sig_4$beta)),ylim=c(0,90),xlim=c(-0.01,0.08),main="Empirical Bayes Method")
lines(density(abs(out_EB_sig_4$beta_EB)),col="orange")
lines(density(abs(true_beta[out_EB_sig_4$rsid])),col="purple")
```



**Important to note:** the conditional likelihood method adjusts each statistic one at a time, and so summary statistics of one individual SNP could easily be inputted into the function. However, both empirical Bayes method and FDR Inverse Quantile Transformation require many SNPs, especially for the empirical Bayes method as the more information given to the function, the more accurate it will be at modelling the distribution and thus, making adjustments. 





