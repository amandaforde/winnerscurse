---
title: "Winner's Curse Adjustment Methods for GWAS summary statistics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Winner's Curse Adjustment Methods for GWAS summary statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(winnerscurse)
```


This package has been designed to provide easy access to published methods which aim to correct for winner's curse, using GWAS summary statistics. The winner's curse is a statistical effect resulting in the exaggeration of SNP-trait association estimates in the sample in which these associations were discovered. With merely estimates of the association, $\beta$, and corresponding standard error, $\text{se}(\beta)$, for each SNP, this package permits users to implement adjustment methods to obtain less biased estimates of the true $\beta$ values. Methods can be applied to data relating to both quantitative and binary traits.

In order to demonstrate how this package can be used, we first create a toy data set and subsequently, illustrate how a user could engage with each of the package's functions using this data set. The methods currently available for implementation are: 

1. Conditional Likelihood methods - [Ghosh et al. (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/)
2. Empirical Bayes method - [Ferguson et al. (2013)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/) 
3. FDR Inverse Quantile transformation method - [Bigdeli et al. (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5013908/) 



## Creating a toy data set

- We wish to generate summary statistics for 1 million SNPs, in which there exists a polygenic background of 10,000 SNPs while all others have no effects.

- We specify:
  1. the total number of SNPs
  2. the number of SNPs truly associated with the trait
  3. the number of samples
  4. the minor allele frequency of each SNP 
  5. the variance explained in the trait by all SNPs, $h^2$ 

- With these specifications, we can obtain quite realistic summary statistics as shown below.

- The summary statistics have been arranged here in a suitable way for each method - in the form of a **data frame with columns `rsid`, `beta` and `se`.** 

```{r}
set.seed(1998)

n_snps <- 10^6
effect_snps <- 10000
n_samples <- 30000

maf <- runif(n_snps,0.01,0.5)
se <- 1/sqrt(2*n_samples*maf*(1-maf))

true_beta <- rnorm(effect_snps,0,1)
h2 <- 0.7 # variance explained by effect SNPs
var_y <- sum(2*maf[1:effect_snps]*(1-maf[1:effect_snps])*true_beta^2)/h2

true_beta <- true_beta/sqrt(var_y) # scaling to represent a phenotype with variance 1
true_beta <- c(true_beta, rep(0,n_snps-effect_snps))

summary_stats <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se),se=se)

head(summary_stats)
```

## Method 1: Conditional Likelihood - Ghosh et al. (2008)

- The function `conditional_likelihood` requires a data frame, in the form described above, and a value for `alpha`, the significance threshold used in the GWAS. 

- As the conditional likelihood methods have been designed to be only used for SNPs which are deemed significant, the data frame returned contains only SNPs that have $p$-values below the inputted genome-wide significance threshold value, `alpha`.

- If no SNPs are detected as significant, the function merely returns the inputted data frame. 

- The returned data frame has SNPs ordered based on their $p$-values from smallest to largest, or equivalently, in descending order of $\mid z \mid$ in which $z$ is the estimated value for $\beta$ divided by its standard error. 

- We implement the function on our toy data set as follows, choosing a significance threshold of `5e-8`:

```{r}

out_CL <- conditional_likelihood(summary_data = summary_stats, alpha = 5e-8) 
head(out_CL)
```

- Three columns have been added to the right of the inputted data frame: each represents a correction method suggested in [Ghosh et al. (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/). The first, `beta.cl1`, is referred to as the conditional MLE while the second, `beta.cl2`, is the mean of the normalized conditional likelihood. The third, `beta.cl3` is merely the average of the first two, known as the compromise estimator. 

- Ghosh et al. (2008) noted that for instances in which the true $\beta$ value is close to zero, `beta.cl2` often has greater mean squared error than `beta.cl1` but for true $\beta$ values further from zero, `beta.cl2` performs better. Thus, it was suggested that `beta.cl3` be used to combine the strengths of these two estimators. However, this function, `conditional_likelihood`, outputs values for all three estimators in order to allow the user to make their own decision on which they believe to be the most appropriate. 


## Method 2: Empirical Bayes - Ferguson et al. (2013)

- The function `empirical_bayes` implements the Empirical Bayes method for correcting for Winner's Curse, detailed in [Ferguson et al. (2013)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/). 

- The function has one argument, `summary_data`, which is the data frame containing `rsid`, `beta` and `se` columns. 

- It returns this data frame but with a fourth column, `beta_EB` in which the adjusted estimates of this procedure have been added. 

- As the Empirical Bayes method makes adjustments to all SNPs, not only those that have been considered significant, this data frame contains all SNPs.  

- **Note:** The conditional likelihood methods adjust each statistic one at a time, and so summary statistics of one individual SNP can easily be inputted into the function. However, the Empirical Bayes method requires many SNPs as the more information given to the function, the more accurate it will be at modelling the distribution and thus, making adjustments. Thus, if `summary_data` contains information related to only one SNP, `empirical_bayes` will merely return the inputted data frame. 

- Below is a demonstration of using `empirical_bayes` with our toy data set:

```{r}
out_EB <- empirical_bayes(summary_data = summary_stats)
head(out_EB)
out_EB[31:37,]
```


- Unfortunately, the Empirical Bayes estimator is known to perform poorly in the extreme tails of the distribution. This explains why we witness that none of the $\beta$ values of the top 6 significant SNPs have been adjusted. However, after having a look at significant SNPs that do not have as extreme $z$-statistics, we can see that reductions in the estimates have been made.
 
- An approach of combining the empirical bayes and the conditional likelihood methods is suggested by Ferguson et al. (2013) to assist overcoming the issues here. 


## Method 3: FDR Inverse Quantile Transformation - Bigdeli et al. (2016)

- The function `FDR_IQT` implements the winner's curse adjustment method detailed in [Bigdeli et al. (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5013908/). 

- Similar to the approaches above, the function requires a data frame with three columns `rsid`, `beta` and `se`. 

- It also has an argument `min_pval`, for which the default setting is `1e-15`. This is included in order to avoid zero $p$-values which can prove problematic for the function when evaluating `qnorm()`. Furthermore, due to the nature of winner's curse, it is in fact rare that observations with $\mid z \mid > 8$ are biased. 

- The function outputs a data frame similar to that inputted with an additional column containing the adjusted estimate, `beta_FIQT`, and again, the SNPs have been reordered according to their individual $z$-statistics. 

```{r}
out_FIQT <- FDR_IQT(summary_data = summary_stats)
head(out_FIQT)
```



## Comparing Results

We can gain a visual insight into the adjustments made by these functions by plotting the adjusted values along with the naive estimates and the true estimates as follows: 

```{r, fig.height=4.5,fig.width=6.5}

out_EB_sig <- out_EB[2*(1-pnorm(abs(out_EB$beta/out_EB$se))) < 5e-8,]
out_FIQT_sig <- out_FIQT[2*(1-pnorm(abs(out_FIQT$beta/out_FIQT$se))) < 5e-8,]
  
plot(density(abs(out_EB_sig$beta)),ylim=c(0,80),xlim=c(-0.01,0.09),main="Comparing Winner's Curse Methods")
lines(density(abs(out_EB_sig$beta_EB)),col="red")
lines(density(abs(out_FIQT_sig$beta_FIQT)),col="green")
lines(density(abs(out_CL$beta.cl1)),col="blue")
lines(density(abs(out_CL$beta.cl2)),col="orange")
lines(density(abs(out_CL$beta.cl3)),col="purple")
lines(density(abs(true_beta[out_CL$rsid])),col="pink")
legend(-0.008, 77.5, legend=c("beta_naive", "beta_EB","beta_FIQT","beta.cl1","beta.cl2","beta.cl3","true_beta"),
       col=c("black","red","green", "blue","orange","purple","pink"),lty=1)

```


The 'tails' issue of the Empirical Bayes method is evident in the above plot, and so, we also look at the Empirical Bayes adjusted estimates on their own but in which we include SNPs with $p$-values less than `5e-4`. It is evident from the plot below that the `empirical_bayes` function does result in a reduction of the naive $\beta$ estimates with the density of the adjusted estimates much closer to that of the true values. 

```{r, fig.height=4.5, fig.width=6.5}
out_EB_sig_4 <- out_EB[2*(1-pnorm(abs(out_EB$beta/out_EB$se))) < 5e-4,]
plot(density(abs(out_EB_sig_4$beta)),ylim=c(0,105),xlim=c(-0.01,0.08),main="Empirical Bayes Method")
lines(density(abs(out_EB_sig_4$beta_EB)),col="orange")
lines(density(abs(true_beta[out_EB_sig_4$rsid])),col="purple")
legend(0.05,80,legend=c("beta_naive","beta_EB","true_beta"),col=c("black","orange","purple"),lty=1)
```



$~$
$~$

**TO DO:** 

- *Implement combination approach of Empirical Bayes and Conditional Likelihood method suggested in Ferguson et al. (2013) as Empirical Bayes is known to perform poorly in the tails of the distribution.*

- *Possibly implement Bayesian Model Averaging - Xu et al. (2011) and BR-squared for summary statistics.*








