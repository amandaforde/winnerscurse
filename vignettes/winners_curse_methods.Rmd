---
title: "Obtaining adjusted $\\hat\\beta$ estimates"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Obtaining adjusted $\\hat\\beta$ estimates}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(winnerscurse)
```


In order to demonstrate how this package can be used to obtain new adjusted $\hat\beta$ estimates, we first create a toy data set and subsequently, illustrate how a user could engage with each of the package's functions using this data set. The methods currently available for implementation are: 

1. Conditional Likelihood methods - [Ghosh *et al.* (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/)
2. Empirical Bayes method - [Ferguson *et al.* (2013)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/) 
3. FDR Inverse Quantile transformation method - [Bigdeli *et al.* (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5013908/) 
4. Bootstrap method - adapted from [Faye *et al.* (2011)](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4228)

## Creating a toy data set

- We wish to generate summary statistics for 1 million SNPs, in which there exists a polygenic background of 10,000 SNPs while all others have no effects.

- We specify:
  1. the total number of SNPs
  2. the number of SNPs truly associated with the trait
  3. the number of samples
  4. the minor allele frequency of each SNP 
  5. the variance explained in the trait by all SNPs, $h^2$ 

- With these specifications, we can obtain quite realistic summary statistics as shown below.

- The summary statistics have been arranged here in a suitable way for each method - in the form of a **data frame with columns `rsid`, `beta` and `se`.** 

```{r}
set.seed(1998)

n_snps <- 10^6
effect_snps <- 10000
n_samples <- 30000

maf <- runif(n_snps,0.01,0.5)
se <- 1/sqrt(2*n_samples*maf*(1-maf))

true_beta <- rnorm(effect_snps,0,1)
h2 <- 0.7 # variance explained by effect SNPs
var_y <- sum(2*maf[1:effect_snps]*(1-maf[1:effect_snps])*true_beta^2)/h2

true_beta <- true_beta/sqrt(var_y) # scaling to represent a phenotype with variance 1
true_beta <- c(true_beta, rep(0,n_snps-effect_snps))

summary_stats <- data.frame(rsid=seq(1,n_snps),beta=rnorm(n=n_snps,mean=true_beta,sd=se),se=se)

head(summary_stats)
```

## Method 1: Conditional Likelihood - Ghosh *et al.* (2008)

- The function `conditional_likelihood` requires a data frame, in the form described above, and a value for `alpha`, the significance threshold used in the GWAS. 

- As the conditional likelihood methods have been designed to be only used for SNPs which are deemed significant, the data frame returned contains only SNPs that have $p$-values below the inputted genome-wide significance threshold value, `alpha`.

- If no SNPs are detected as significant, the function merely returns the inputted data frame. 

- The returned data frame has SNPs ordered based on their $p$-values from smallest to largest, or equivalently, in descending order of $\mid z \mid$ in which $z$ is the estimated value for $\beta$ divided by its standard error. 

- We implement the function on our toy data set as follows, choosing a significance threshold of `5e-8`:

```{r}
out_CL <- conditional_likelihood(summary_data = summary_stats, alpha = 5e-8) 
head(out_CL)

```

- Three columns have been added to the right of the inputted data frame: each represents a correction method suggested in [Ghosh *et al.* (2008)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2665019/). The first, `beta.cl1`, is referred to as the conditional MLE while the second, `beta.cl2`, is the mean of the normalized conditional likelihood. The third, `beta.cl3` is merely the average of the first two, known as the compromise estimator. 

- Ghosh *et al.* (2008) noted that for instances in which the true $\beta$ value is close to zero, `beta.cl2` often has greater mean squared error than `beta.cl1` but for true $\beta$ values further from zero, `beta.cl2` performs better. Thus, it was suggested that `beta.cl3` be used to combine the strengths of these two estimators. However, this function, `conditional_likelihood`, outputs values for all three estimators in order to allow the user to make their own decision on which they believe to be the most appropriate. 


## Method 2: Empirical Bayes - Ferguson *et al.* (2013)

- The function `empirical_bayes` implements the Empirical Bayes method for correcting for Winner's Curse, detailed in [Ferguson *et al.* (2013)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4048064/), with a slight modification. 

- The function has one argument, `summary_data`, which is the data frame containing `rsid`, `beta` and `se` columns. 

- It returns this data frame but with a fourth column, `beta_EB` in which the adjusted estimates of this procedure have been added. 

- As the Empirical Bayes method makes adjustments to all SNPs, not only those that have been considered significant, this data frame contains all SNPs.  

- **Note:** The conditional likelihood methods adjust each statistic one at a time, and so summary statistics of one individual SNP can easily be inputted into the `conditional_likelihood` function. However, the Empirical Bayes method requires many SNPs as the more information given to the function, the more accurate it will be at modelling the distribution and thus, making adjustments. Thus, if `summary_data` contains information related to only one SNP, `empirical_bayes` will merely return the inputted data frame. 

- Below is a demonstration of using `empirical_bayes` with our toy data set:

```{r}
out_EB <- empirical_bayes(summary_data = summary_stats)
head(out_EB)

```


- Unfortunately, the Empirical Bayes estimator described by Ferguson *et al.* (2013) is known to perform poorly in the extreme tails of the distribution. Therefore, it is quite possible that a lack of adjustment for the $\beta$ values of SNPs with the most extreme $z$-statistics could be witnessed with this method. A somewhat ad hoc approach to overcome this issue of combining the Empirical Bayes and the conditional likelihood methods was suggested by Ferguson *et al.* (2013). 

- In order to ensure that shrinkage does occur for the $\beta$ values of these extreme SNPs, it was decided that the following modification would be added to the `empirical_bayes` function detailed here. The basis function of the natural cubic spline has been altered so that the boundary knots are no longer the most extreme $z$-values. Instead, the lower boundary knot is defined as the $10^{\text{th}}$ $z$-statistic when the $z$-statistics lie in increasing order while the upper boundary knot is the $10^{\text{th}}$ $z$-statistic when the $z$-statistics have been arranged in decreasing order. The natural cubic spline is then merely linear beyond these boundary knots. This assists in fixing the 'tails' issue discussed above. We see that the reduction of the estimated $\beta$ values for all of the top 6 most significant SNPs has occurred. 



## Method 3: FDR Inverse Quantile Transformation - Bigdeli *et al.* (2016)

- The function `FDR_IQT` implements the winner's curse adjustment method detailed in [Bigdeli *et al.* (2016)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5013908/). 

- Similar to the approaches above, the function requires a data frame with three columns `rsid`, `beta` and `se`. 

- It also has an argument `min_pval`, for which the default setting is `1e-15`. This is included in order to avoid zero $p$-values which can prove problematic for the function when evaluating `qnorm()`. Furthermore, due to the nature of winner's curse, it is in fact rare that observations with $\mid z \mid > 8$ are biased. 

- The function outputs a data frame similar to that inputted with an additional column containing the adjusted estimate, `beta_FIQT`, and again, the SNPs have been reordered according to their individual $z$-statistics. 

```{r}
out_FIQT <- FDR_IQT(summary_data = summary_stats)
head(out_FIQT)

```


## Method 4: Bootstrap - adapted from Faye *et al.* (2011)

- Inspired by the winner's curse adjustment method detailed in [Faye *et al.* (2011)](https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4228), the function `BR_ss` implements a computationally efficient bootstrap method which is suitable for use with summary statistics. 

- `BR_ss` takes a data frame `summary_data` with three columns: `rsid`, `beta`, `se`, and returns the inputted data frame along with the adjusted estimate for $\beta$, `beta_BR_ss`.

$~$

**Method description:**

- First, the $N$ SNPs are arranged according to their naive $z$-statistics, $\frac{\hat\beta}{\text{se}(\hat\beta)}$, in descending order and given rank $k$. Thus, the SNP with the largest positive original $z$-statistic is given rank $k=1$ while the SNP with the largest negative original $z$-statistic receives rank $k=N$. 

- Following this, a parametric bootstrap is performed in which a value $\hat\beta^{\text{b}}_{(k)}$ is simulated for SNP $k$ from the distribution: 
$$\hat\beta^{\text{b}}_{(k)} \sim N(\hat\beta_{(k)}, \text{se}(\hat\beta_{(k)})).$$ Upon obtaining $\hat\beta^{\text{b}}$, SNPs undergo a second ordering in which they are arranged based on $z^{\text{b}}$-statistics and receive a rank $A(k)$ accordingly. The $z^{\text{b}}_{(k)}$-statistic of SNP $k$ is equal to $\frac{\hat\beta^{\text{b}}_{(k)}}{\text{se}(\hat\beta_{(k)})}$. 

- An estimate for the bias correction value of SNP $k$ is given as:
$$\text{bias}_{(k)} = \frac{\hat\beta^{\text{b}}_{A(k)} - \hat\beta^{\text{oob}}_{A(k)}}{\text{se}(\hat\beta_{A(k)})} = \frac{\hat\beta^{\text{b}}_{A(k)} - \hat\beta_{A(k)}}{\text{se}(\hat\beta_{A(k)})}, $$in which $\hat\beta^{\text{b}}_{A(k)}$ is the bootstrap value of the SNP ranked in position $k$ in the second ordering, $\hat\beta^{\text{oob}}_{A(k)} = \hat\beta_{A(k)}$ is that same SNP's naive $\beta$ estimate and $\text{se}(\hat\beta_{A(k)})$ it's standard error. 

- Next, a LOESS curve is fitted to the data in which $z_{(k)}$-statistics are considered as the inputs and $\text{bias}_{(k)}$, their corresponding outputs. As we are often dealing with very large numbers of SNPs, a LOESS curve is only fitted for the most significant SNPs. The `BR_ss` function currently fits one LOESS curve for the SNPs ranked in the top $1\%$ of positions and another one for those ranked in the bottom $1\%$. A linear regression is used for all other SNPs. The predicted values from this process provides $\text{bias}^{*}_{(k)}$ for each SNP. Using $\text{bias}^{*}_{(k)}$ instead of $\text{bias}_{(k)}$ results in a faster approach with increased accuracy as only one bootstrap iteration for each SNP is sufficient for competitive performance. 

- Finally, the new estimate for $\beta$ for SNP $k$ is defined as follows:
$$\hat\beta_{(k)}^{*} = \hat\beta_{(k)} - \text{se}(\hat\beta_{(k)}) \cdot  \text{bias}^{*}_{(k)}.$$


```{r}
out_BR_ss <- BR_ss(summary_data = summary_stats)
head(out_BR_ss)
```





## Comparing Results

As we have simulated our data set, we have the ability to quantify the amount of bias present in the original naive $\beta$ estimates and also, in our adjusted estimates for each method. We can use measures such as the sum of squared differences and mean of absolute differences to assess this bias. First, we take a look at the conditional likelihood method as follows, in which only SNPs that are originally deemed significant are included: 

```{r}
sq_diff_1 <- data.frame(naive = sum((true_beta[out_CL$rsid] - out_CL$beta)^2), beta.cl1 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl1)^2),  beta.cl2 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl2)^2),  beta.cl3 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl3)^2))
sq_diff_1

mean_abs_diff_1 <- data.frame(naive = mean(abs(true_beta[out_CL$rsid] - out_CL$beta)), beta.cl1 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl1)),  beta.cl2 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl2)),  beta.cl3 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl3)))
mean_abs_diff_1
```

For both quantities, the values for all three conditional likelihood methods are less than that of the naive estimate. 


Next, we investigate the other three methods described above which take into account all SNPs, not just those with $p$-values less than a specified threshold. 

```{r}
sq_diff_2 <- data.frame(naive = sum((true_beta[out_EB$rsid] - out_EB$beta)^2), beta_EB = sum((true_beta[out_EB$rsid] - out_EB$beta_EB)^2), beta_FIQT = sum((true_beta[out_FIQT$rsid] - out_FIQT$beta_FIQT)^2), beta_BR_ss = sum((true_beta[out_BR_ss$rsid] - out_BR_ss$beta_BR_ss)^2))
sq_diff_2

mean_abs_diff_2 <- data.frame(naive = mean(abs(true_beta[out_EB$rsid] - out_EB$beta)), beta_EB = mean(abs(true_beta[out_EB$rsid] - out_EB$beta_EB)), beta_FIQT = mean(abs(true_beta[out_FIQT$rsid] - out_FIQT$beta_FIQT)), beta_BR_ss = mean(abs(true_beta[out_BR_ss$rsid] - out_BR_ss$beta_BR_ss)))
mean_abs_diff_2

```
Again, we see that in both cases, the value associated with the naive estimate is greatest. This assures us that the new adjusted estimates provided by each method are indeed less biased than the original $\beta$ estimate. 


Finally, we can compare the performance of all methods for the SNPs with raw $p$-values less than `5e-8`.
```{r}
out_EB_sig <- out_EB[2*(1-pnorm(abs(out_EB$beta/out_EB$se))) < 5e-8,]
out_FIQT_sig <- out_FIQT[2*(1-pnorm(abs(out_FIQT$beta/out_FIQT$se))) < 5e-8,]
out_BR_ss_sig <- out_BR_ss[2*(1-pnorm(abs(out_BR_ss$beta/out_BR_ss$se))) < 5e-8,]


sq_diff_3 <- data.frame(naive = sum((true_beta[out_CL$rsid] - out_CL$beta)^2), beta.cl1 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl1)^2),  beta.cl2 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl2)^2),  beta.cl3 = sum((true_beta[out_CL$rsid] - out_CL$beta.cl3)^2), beta_EB = sum((true_beta[out_EB_sig$rsid] - out_EB_sig$beta_EB)^2), beta_FIQT = sum((true_beta[out_FIQT_sig$rsid] - out_FIQT_sig$beta_FIQT)^2), beta_BR_ss = sum((true_beta[out_BR_ss_sig$rsid] - out_BR_ss_sig$beta_BR_ss)^2))
sq_diff_3

mean_abs_diff_3 <- data.frame(naive = mean(abs(true_beta[out_CL$rsid] - out_CL$beta)), beta.cl1 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl1)),  beta.cl2 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl2)),  beta.cl3 = mean(abs(true_beta[out_CL$rsid] - out_CL$beta.cl3)), beta_EB = mean(abs(true_beta[out_EB_sig$rsid] - out_EB_sig$beta_EB)), beta_FIQT = mean(abs(true_beta[out_FIQT_sig$rsid] - out_FIQT_sig$beta_FIQT)), beta_BR_ss = mean(abs(true_beta[out_BR_ss_sig$rsid] - out_BR_ss_sig$beta_BR_ss)))
mean_abs_diff_3
```


$~$
$~$

In addition, we can gain a visual insight into the adjustments made by these functions by plotting the adjusted *absolute* values along with the naive estimates and the true *absolute* $\beta$ values of SNPs which are originally deemed as significant, i.e. those with raw $p$-values less than `5e-8`, as follows: 

```{r, fig.height=4.5,fig.width=6.5, warning=FALSE}
library("RColorBrewer")
col <- brewer.pal(8,"Dark2")
  
plot(density(abs(out_EB_sig$beta)),ylim=c(0,80),xlim=c(-0.01,0.09),main="Comparing Winner's Curse Methods",col=col[1],lwd=2)
lines(density(abs(true_beta[out_CL$rsid])),col=col[2],lwd=2)
lines(density(abs(out_FIQT_sig$beta_FIQT)),col=col[3],lwd=2)
lines(density(abs(out_CL$beta.cl1)),col=col[4],lwd=2)
lines(density(abs(out_CL$beta.cl2)),col=col[5],lwd=2)
lines(density(abs(out_CL$beta.cl3)),col=col[6],lwd=2)
lines(density(abs(out_EB_sig$beta_EB)),col=col[7],lwd=2)
lines(density(abs(out_BR_ss_sig$beta_BR_ss)),col=col[8],lwd=2)
legend(-0.008, 77.5, legend=c("beta_naive", "true_beta","beta_FIQT","beta.cl1","beta.cl2","beta.cl3","beta_EB", "beta_BR_ss"),
       col=col,lty=1,lwd=2)

```




$~$
$~$

**TO DO:** 

- *Estimation of variance/standard errors for the adjusted $\beta$ estimates given by all methods.*
- *How to let data make decision on number of SNPs to use in the extremes for `empirical_bayes` and number of SNPs used in loess in `BR_ss`?*






